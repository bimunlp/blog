<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>chapter 6 Hidden Variables</title>
      <link href="/2019/01/17/chapter-6-Hidden-Variables/"/>
      <url>/2019/01/17/chapter-6-Hidden-Variables/</url>
      
        <content type="html"><![CDATA[<p>There are situations where parts of the outputs are not available in training data. In such case, we need to consider models that contain hidden variables.</p><a id="more"></a><!-- toc --><ul><li><a href="#chapter-6">chapter 6</a></li><li><a href="#hidden-variables">Hidden Variables</a><ul><li><a href="#61">6.1</a></li><li><a href="#expectation-maximization">Expectation Maximization</a></li><li><a href="#em-algorithm-for-k-means-clustering">EM algorithm for K-means clustering</a></li><li><a href="#611-hard-em">6.1.1 Hard EM</a></li><li><a href="#hard-em">Hard EM</a></li><li><a href="#k-means-vs-hard-em">K-means v.s. Hard EM</a></li><li><a href="#612-soft-em">6.1.2 Soft EM</a></li><li><a href="#613-relationships-between-supervised-mle-and-em">6.1.3 Relationships between supervised MLE and EM</a></li><li><a href="#62">6.2</a></li><li><a href="#em-derivation-and-kl-divergence">EM Derivation and KL-Divergence</a></li><li><a href="#622-em-deriviation-using-numerical-optimization">6.2.2 EM deriviation using Numerical Optimization</a></li><li><a href="#63">6.3</a></li><li><a href="#application-of-em">Application of EM</a></li><li><a href="#em-algorithm-summary">EM Algorithm Summary</a></li><li><a href="#631-unsupervised-naive-bayes-model">6.3.1 Unsupervised Naive Bayes model</a></li><li><a href="#632-ibm-model-1">6.3.2 IBM Model 1</a></li><li><a href="#word-alignment">Word alignment</a></li><li><a href="#em-training">EM training</a></li><li><a href="#ibm-model-1">IBM model 1</a></li><li><a href="#633-probabilistic-latent-semantic-analysis">6.3.3 Probabilistic Latent Semantic Analysis</a></li><li><a href="#plsa-applications">PLSA Applications</a></li><li><a href="#summary">Summary</a></li></ul></li></ul><!-- tocstop --><h1><span id="chapter-6">chapter 6</span></h1><h1><span id="hidden-variables">Hidden Variables</span></h1><hr><p><img src="/home/scott/Downloads/hidden.png" alt=""><br><strong>Expextation maximization (EM)</strong></p><ul><li>unsupervised Naive Bayes model</li><li>IBM model 1</li><li>probabilistic latent semantic analysis (PLSA) model</li></ul><hr><h2><span id="61">6.1</span></h2><h2><span id="expectation-maximization">Expectation Maximization</span></h2><hr><p>Observation on K-means clustering<br><img src="/home/scott/Downloads/kmean.png" alt=""><br>EM processes:</p><ul><li>expectation step: <ul><li>random point-cluster assignment</li></ul></li><li>maximization step:<ul><li>minimizes vector distances to the centroids</li></ul></li></ul><hr><h2><span id="em-algorithm-for-k-means-clustering">EM algorithm for K-means clustering</span></h2><p>#</p><ul><li>Observed points : $O = \left. \left\{ o _ { i } \right\} \right| _ { i = 1 } ^ { N }$</li><li>Learning objective : minimising $L ( O ) = \sum _ { i = 1 } ^ { N } \sum _ { k = 1 } ^ { K } \mathbf { h } _ { i k } \left| o _ { i } - c _ { k } \right| ^ { 2 }$ ,where $c_{k}$ is  the centroid of cluster $k$, and $h_{ik}$ is an indicator variable.</li><li>Notations :<ul><li>Hidden variable $H = \left. \left\{ \mathbf { h } _ { i k } \right\} \right| _ { i = 1 , k = 1 } ^ { N , K }$ ,</li><li>Parameter  $\Theta = \left. \left\{ c _ { k } \right\} \right| _ { k = 1 } ^ { K }$ </li></ul></li></ul><hr><ul><li>Training process :<ul><li>Iteration over $H$: $H ^ { t } \leftarrow { \arg \min } \sum _ { i = 1 } ^ { N } \sum _ { k = 1 } ^ { K } \mathbf { h } _ { i k } \left| o _ { i } - c _ { k } ^ { t } \right| ^ { 2 }$</li><li>Iteration over $\Theta$: $\Theta ^ { t + 1 } \leftarrow { \arg \min } \sum _ { i = 1 } ^ { N } \sum _ { k = 1 } ^ { K } \mathbf { h } _ { i k } ^ { t } \left| o _ { i } - c _ { k } \right| ^ { 2 }$</li></ul></li><li>The optimal value : $c _ { k } ^ { t + 1 } = \frac { \sum _ { i = 1 } ^ { N } \mathbf { h } _ { i k } ^ { t } O _ { i } } { \sum _ { i = 1 } ^ { N } \mathbf { h } _ { i k } ^ { t } }$ , which is the average of all points in cluster $k$.</li></ul><hr><h2><span id="611-hard-em">6.1.1 Hard EM</span></h2><p>General form of EM algorithm<br>#<br>Notations:</p><ul><li>Observed data $O$</li><li>Hidden data $H$</li><li>Model parameter $\Theta$</li><li>Model $P(O,H|\Theta)$</li></ul><p>Trainging objective with hidden variables:</p><ul><li>Maximising $L(\Theta)=logP(O|\Theta)=log\sum_{H}P(O,H|\Theta)$</li></ul><hr><h2><span id="hard-em">Hard EM</span></h2><p><img src="/home/scott/Desktop/hardem.png" alt=""><br>Using a single optimal configuration of $H$, hard EM optimizes<br>$L ( \Theta ) = \log \max _ { H } P ( O , H | \Theta )$<br>The optimum $\Theta^{<em>}$ given by hard EM is<br>$\Theta^</em>\leftarrow argmax_{\Theta}max_{H}logP(O|\Theta)$</p><hr><h2><span id="k-means-vs-hard-em">K-means v.s. Hard EM</span></h2><p><img src="/home/scott/Downloads/vs.png" alt=""><br>K-means is a type of hard EM algorithm.</p><hr><h2><span id="612-soft-em">6.1.2 Soft EM</span></h2><p>Soft EM considers all possible values of hidden variables.<br><img src="/home/scott/Desktop/softem.png" alt=""><br>$P(h|o_{i},\Theta^{t}),h \in H$ is the assignment distribution of $H$.<br>$Q(\Theta,\Theta^t)$ is called the <strong>Q-function</strong>.</p><p>Hard EM is a special case of soft EM.</p><hr><h2><span id="613-relationships-between-supervised-mle-and-em">6.1.3 Relationships between supervised MLE and EM</span></h2><p>We can turn EM algorithm to MLE in supervised settings.</p><ul><li>suppose each $o_{i}$ has a supervised label $y_{i}$</li><li><p>defining<br>$P ( h | o _ { i } , \Theta ^ { t } ) = \left\{ \begin{array} { l } { 1 \text { if } h = y _ { i } } \ { 0 \text { otherwise } } \end{array} \right.$</p><p>$\begin{aligned} Q \left( \Theta , \Theta ^ { t } \right) &amp; = \sum _ { i = 1 } ^ { N } \sum _ { \mathbf { h } \in H } P ( \mathbf { h } | o _ { i } , \Theta ^ { t } ) \log P \left( o _ { i } , \mathbf { h } | \Theta \right) \ &amp; = \sum _ { i = 1 } ^ { N } \log P \left( o _ { i } , y _ { i } | \Theta \right) \end{aligned}$</p><p>which is exactly the maximum log-likelihood training objective.</p></li></ul><hr><h2><span id="62">6.2</span></h2><h2><span id="em-derivation-and-kl-divergence">EM Derivation and KL-Divergence</span></h2><hr><p> Jensen inequality<br> <img src="/home/scott/Downloads/Convex_b.png" alt=""><br> for convex functions, $E(g(\mu))\ge g(E(\mu))$,<br> for concave functions, $E(g(\mu))\le g(E(\mu)).$</p><hr><p> Using Jensen inequality<br> $L(\Theta)=log\sum_{h}P(O,h|\Theta)\ge$ $\sum _ { h } P _ { C } ( h ) \log \frac { P ( O , h | \Theta ) } { P _ { C } ( h ) }=F(\Theta,Pc)$,<br> $F(\Theta,Pc)$ is a lower bound of $L(\Theta)$</p><p>Also,<br> $F(\Theta,Pc)= L ( \Theta ) - K L \left( P _ { C } ( h ) , P ( h | O , \Theta ) \right)$</p><blockquote><p>KL-divergence is always non-negative</p><blockquote><p>$KL(P,Q)$ is zero if and only if $P=Q$</p><p>To make the bound as tight as possible,<br>$Pc(h)=P(h|O,\Theta)$<br>In this scenario, $F(\Theta,Pc)=L(\Theta)$</p></blockquote></blockquote><hr><h2><span id="622-em-deriviation-using-numerical-optimization">6.2.2 EM deriviation using Numerical Optimization</span></h2><p> Another way to maximizes $F(\Theta,Pc)$<br> <strong>Coordinate ascent</strong></p><ul><li>Expectation step.<br>finds a optimum distributioon $Pc(H)$ that maximizes $F(\Theta^t,Pc)$</li><li>Maximization step.<br>finds the optimum $\Theta^{t+1}$ for $F(\Theta,P^{t+1}_{c})$</li><li>Convergence.<br>after every iteration of E-step and M-step,<br> $L(\Theta^{t+1})-L(\Theta)\ge0$,<br> $L(\Theta)$ is a monotonically increasing function, EM is guaranteed to converge to local optimums.</li></ul><hr><h2><span id="63">6.3</span></h2><h2><span id="application-of-em">Application of EM</span></h2><hr><h2><span id="em-algorithm-summary">EM Algorithm Summary</span></h2><p> To apply EM to a certain task, we need three particular steps:</p><ol><li>obtain the complete data likelihood $P(O,H|\Theta)$</li><li>compute $P(H|O,\Theta)$<br>$P(H|O,\Theta)=\frac{P(O,H|\Theta)}{P(O|\Theta)}$</li><li>maximize $Q(\Theta,\Theta^{t})$<br>$Q \left( \Theta , \Theta ^ { t } \right) = \sum _ { h } P ( h | O , \Theta ^ { t } ) \log P ( O , h | \Theta )$</li></ol><hr><h2><span id="631-unsupervised-naive-bayes-model">6.3.1 Unsupervised Naive Bayes model</span></h2><p> Hidden variables h: output document class,<br> for document $d_{i}$, the complete data likelihood is </p><p>$P \left( d _ { i } , \mathbf { h } | \Theta ^ { t } \right) = P ( \mathbf { h } | \Theta ^ { t } ) P \left( d _ { i } | \mathbf { h } , \Theta ^ { t } \right) = P ( \mathbf { h } | \Theta ^ { t } ) \prod _ { i = 1 } ^ { \left| d _ { i } \right| } P \left( w _ { i } | \mathbf { h } , \Theta ^ { t } \right)$</p><p>Then we can calculate </p><p>$P ( \mathbf { h } | d _ { i } , \Theta ^ { t } ) = \frac { P \left( d _ { i } , \mathbf { h } | \Theta ^ { t } \right) } { \sum _ { \mathbf { h } } P \left( d _ { i } , \mathbf { h } | \Theta ^ { t } \right) } = \frac { P ( \mathbf { h } | \Theta ^ { t } ) \prod _ { i = 1 } ^ { \left| d _ { i } \right| } P \left( w _ { i } | \mathbf { h } , \Theta ^ { t } \right) } { \sum _ { \mathbf { h } } P ( \mathbf { h } | \Theta ^ { t } ) \prod _ { i = 1 } ^ { \left| d _ { i } \right| } P \left( w _ { i } | \mathbf { h } , \Theta ^ { t } \right) }$</p><p>Maximize the expectation,</p><p>$Q \left( \Theta , \Theta ^ { t } \right) = \sum _ { i = 1 } ^ { N } \sum _ { \mathbf { h } } P ( \mathbf { h } | d _ { i } , \Theta ^ { t } ) \log P \left( d _ { i } , \mathbf { h } | \Theta \right)$</p><hr><p>Finding $\arg \max _ { \Theta } Q \left( \Theta , \Theta ^ { t } \right)$ s.t. $\sum _ { \mathbf { h } } P ( \mathbf { h } | \Theta ) = 1$ and $\sum _ { \mathbf { w } \in V } P ( \mathbf { w } | \mathbf { h } , \Theta ) = 1$</p><p>Using Lagrangian multiplier,</p><p>We derive that,</p><p>$P ( \mathbf { h } | \Theta ) = \frac { \sum _ { i = 1 } ^ { N } P ( \mathbf { h } | d _ { i } , \Theta ^ { t } ) } { N }$</p><p>$P ( \mathbf { w } | \mathbf { h } , \Theta ) =$ $\frac { \sum _ { i = 1 } ^ { N } P ( \mathbf { h } | d _ { i } , \Theta ^ { t } ) \sum _ { j = 1 } ^ { \left| d _ { i } \right| } \delta \left( w _ { j } , w \right) } { \sum _ { i = 1 } ^ { N } P ( \mathbf { h } | d _ { i } , \Theta ^ { t } ) \left| d _ { i } \right| }$</p><p>soft EM can be executed now.</p><hr><h2><span id="632-ibm-model-1">6.3.2 IBM Model 1</span></h2><p>A probabilistic model for Machine Translation (MT) </p><ul><li>source sentence X</li><li>target language translation Y</li><li>source word $X = x _ { 1 } x _ { 2 } \ldots x _ { | X | }$</li><li>Target word $Y = y _ { 1 } y _ { 2 } \dots y _ { | Y | }$</li></ul><p>Bayes rule: $P ( Y | X ) = \frac { P ( X | Y ) P ( Y ) } { P ( X ) } \propto P ( X | Y ) P ( Y )$</p><ul><li>Language model : $P(Y)$  ensure fluency</li><li>Translation model : $P(X|Y)$  ensure adequacy</li></ul><p>Using probability chain rule and assuming each source word $x_{i}$ is conditionally dependent to only one target word $y_{a_{i}}$, we have</p><p>$P(X|Y)$ $= P \left( x _ { 1 } | y _ { a _ { 1 } } \right) P \left( x _ { 2 } | y _ { a _ { 2 } } \right) \ldots P \left( x _ { | X | } | y _ { a _ { | X | } } \right)$</p><hr><h2><span id="word-alignment">Word alignment</span></h2><p>Word alignment $A = \left. \left\{ a _ { i } \right\} \right| _ { i = 1 } ^ { | X | }$ , $a_{i}$ denotes the index of the target word that the $i$-th source word translates to.<br><img src="/home/scott/Desktop/alignment.png" alt=""><br>types of word alignment between sentence translation pairs:</p><p>Monotonic , Non-monotonic , Many-to-one , Null </p><hr><h2><span id="em-training">EM training</span></h2><ul><li>Observation variable: $O=(X,Y)$ ,i.e. sentence translation pairs $D = \left. \left\{ \left( X _ { i } , Y _ { i } \right) \right\} \right| _ { i = 1 } ^ { N }$</li><li>Hidden variable: $H=A$ , i.e. word alignment $A_{i}$</li></ul><p>$\begin{aligned} P ( A , X | Y ) &amp; = P ( A | Y ) P ( X | A , Y ) \ &amp; = \frac { \prod _ { i = 1 } ^ { | X | } P \left( x _ { i } | y _ { a _ { i } } \right) } { # ( A \text { between } X \text { and } Y ) } \ &amp; = \frac { \prod _ { i = 1 } ^ { | X | } P \left( x _ { i } | y _ { a _ { i } } \right) } { ( | Y | + 1 ) ^ { | X | } } \end{aligned}$</p><p>$\begin{aligned} P ( A | X , Y ) &amp; = \frac { P ( A , X | Y ) } { P ( X | Y ) } \ &amp; = \frac { \prod _ { i = 1 } ^ { | X | } P \left( x _ { i } | y _ { a _ { i } } \right) } { \prod _ { i = 1 } ^ { | X | } \sum _ { j = 0 } ^ { | Y | } P \left( x _ { i } | y _ { j } \right) } \ &amp; = \prod _ { i = 1 } ^ { | X | } \frac { P \left( x _ { i } | y _ { a _ { i } } \right) } { \sum _ { j = 0 } ^ { | Y | } P \left( x _ { i } | y _ { j } \right) } \end{aligned}$</p><hr><p>After knowing $P ( A | X , Y )$ and $P ( A , X | Y )$ , we can define the Q-function for sentence translation pair $(X,Y)$:</p><p>$\begin{aligned} Q \left( \Theta , \Theta ^ { t } \right) &amp; = \sum _ { A } P ( A | X , Y , \Theta ^ { t } ) \log P ( A , X | Y , \Theta ) \ &amp; = \sum _ { A } P ( A | X , Y , \Theta ^ { t } ) \log \frac { \prod _ { i = 1 } ^ { | X | } P \left( x _ { i } | y _ { a _ { i } } , \Theta \right) } { ( | Y | + 1 ) ^ { | X | } } \end{aligned}$</p><p>Applying Lagrangian function, </p><p>The expected alignment between a word translation pair </p><p>EXPECTEDALIGN$( \mathbf { x } , \mathbf { y } , X , Y )$ $= \sum _ { A } P ( A | X , Y ) \cdot \sum _ { k = 1 } ^ { | X | } \delta \left( \mathbf { x } , x _ { k } \right) \delta \left( \mathbf { y } , y _ { a _ { k } } \right)$</p><p>$= \frac { P ( \mathbf { x } | \mathbf { y } ) } { \sum _ { j = 0 } ^ { | Y | } P ( \mathbf { x } | y _ { j } ) } \sum _ { i = 1 } ^ { | X | } \delta \left( \mathbf { x } , x _ { i } \right) \sum _ { j = 0 } ^ { | Y | } \delta \left( \mathbf { y } , y _ { j } \right)$</p><p>EXPECTEDALIGN $( \mathbf { x } , \mathbf { y } , X , Y )$ represents a soft count.</p><hr><h2><span id="ibm-model-1">IBM model 1</span></h2><p>Word alignment<br><img src="/home/scott/Desktop/algorithms.png" alt=""><br>IBM models are word-based machine translation models</p><hr><h2><span id="633-probabilistic-latent-semantic-analysis">6.3.3 Probabilistic Latent Semantic Analysis</span></h2><p><img src="/home/scott/Downloads/topic.png" alt=""><br>PLSA is a generative model for document semantic analysis.<br> Topics are hidden variables .</p><ul><li>Document-topic distribution $P(h|d_{i})$</li><li>Topic-word distribution $P(w|h)$</li></ul><hr><p> <img src="/home/scott/Desktop/plsa.png" alt=""><br> For every document $d$, for every position $l$:</p><ol><li>select a document $d_{i}$ from $P(d_{i})$</li><li>generate a topic $h_{l}$ from $P(h|d)$</li><li>generate a word $w_{l}$ from $P(w|h_{l})$</li></ol><p>The complete data likelihood of word-document pair <w,h> under d:<br>$P(w,h|d)=P(h|d)P(w|h,d)$<br>$P ( \mathbf { h } | d _ { i } , \mathbf { w } , \Theta ^ { t } ) = \frac { P \left( \mathbf { h } , d _ { i } , \mathbf { w } | \Theta ^ { t } \right) } { P \left( d _ { i } , \mathbf { w } | \Theta ^ { t } \right) }$</w,h></p><p>$= \frac { P ( \mathbf { h } | d _ { i } , \Theta ^ { t } ) P ( \mathbf { w } | \mathbf { h } , \Theta ^ { t } ) } { \sum _ { \mathbf { h } ^ { \prime } } P \left( \mathbf { h } ^ { \prime } | d _ { i } , \Theta ^ { t } \right) P ( \mathbf { w } | \mathbf { h } ^ { \prime } , \Theta ^ { t } ) }$</p><p>The Q-function:</p><p>$Q \left( \Theta , \Theta ^ { t } \right) = \sum _ { i = 1 } ^ { N } \sum _ { \mathbf { w } \in d _ { i } } \sum _ { \mathbf { h } } P ( \mathbf { h } | d _ { i } , \mathbf { w } , \Theta ^ { t } ) \log P \left( \mathbf { h } , d _ { i } , \mathbf { w } | \Theta \right)$</p><p>$= \sum _ { i = 1 } ^ { N } \sum _ { \mathbf { w } \in V } C \left( \mathbf { w } , d _ { i } \right) \sum _ { \mathbf { h } } P ( \mathbf { h } | d _ { i } , \mathbf { w } , \Theta ^ { t } ) [ \log P ( \mathbf { h } | d _ { i } , \Theta ) + \log P ( \mathbf { w } | \mathbf { h } , \Theta ) ]$</p><hr><p>Define a Lagrangian function</p><p>$\Lambda ( \Theta , \lambda ) = Q \left( \Theta , \Theta ^ { t } \right) - \sum _ { i } \lambda _ { d _ { i } } \left( \sum _ { \mathbf { h } } P ( \mathbf { h } | d _ { i } , \Theta ) - 1 \right)$ $- \sum _ { \mathbf { h } } \lambda _ { \mathbf { h } } \left( \sum _ { \mathbf { w } } P ( \mathbf { w } | \mathbf { h } , \Theta ) - 1 \right)$</p><p>consider $\frac { \partial \Lambda ( \Theta , \lambda ) } { \partial P ( \mathbf { h } | d _ { i } , \Theta ) }=0$ and $\sum _ { \mathbf { h } } P ( \mathbf { h } | d _ { i } , \Theta ) - 1 = 0$</p><p>$P ( \mathbf { h } | d _ { i } , \Theta )=$$\frac { \sum _ { \mathbf { w } \in V } C \left( \mathbf { w } , d _ { i } \right) P ( \mathbf { h } | d _ { i } , \mathbf { w } , \Theta ^ { t } ) } { \sum _ { \mathbf { w } \in V } C \left( \mathbf { w } , d _ { i } \right) }$</p><p>$P ( \mathbf { w } | \mathbf { h } , \Theta ) = \frac { \sum _ { i = 1 } ^ { N } C \left( \mathbf { w } , d _ { i } \right) P ( \mathbf { h } | d _ { i } , \mathbf { w } , \Theta ^ { t } ) } { \sum _ { i = 1 } ^ { N } \sum _ { \mathbf { w } \in V } C \left( \mathbf { w } , d _ { i } \right) P ( \mathbf { h } | d _ { i } , \mathbf { w } , \Theta ^ { t } ) }$</p><hr><h2><span id="plsa-applications">PLSA Applications</span></h2><ul><li>Keyword extraction</li><li>Information retrieval</li><li>Recommendation systems</li></ul><hr><h2><span id="summary">Summary</span></h2><ul><li>The concept of hidden variables</li><li>Hard and soft variations of the Expectation Maximization (EM) algorithm</li><li>THe correlation between EM and MLE for training probabilistic models</li><li>EM for unsupervised text classification</li><li>IBM model 1 for statistical machine translation</li><li>Probabilistic latent semantic allocation</li></ul>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 隐变量 </tag>
            
            <tag> hidden variables </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>什么是深度学习？</title>
      <link href="/2019/01/16/No%202%20%E4%BB%80%E4%B9%88%E6%98%AF%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%9F/"/>
      <url>/2019/01/16/No%202%20%E4%BB%80%E4%B9%88%E6%98%AF%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<p>机器学习与深度学习基础教程</p><p><img src="http://plgrq7ioi.bkt.clouddn.com/cover_1.jpg" alt="cover_1"></p><p>这篇文章将要回答什么是深度学习这个问题．</p><p>整个系列课程会涵盖深度学习领域的众多主题，我们会用很多篇文章来充分解释这些课题，以及它们的应用领域和技术实现。</p><a id="more"></a><!-- toc --><ul><li><a href="#深度学习概念解析">深度学习概念解析</a><ul><li><a href="#什么是深度学习">什么是深度学习？</a></li><li><a href="#深度学习中的深度指的是什么">深度学习中的＂深度＂指的是什么？</a></li></ul></li></ul><!-- tocstop --><p><img src="http://plgrq7ioi.bkt.clouddn.com/image_2.png" alt="image_2"></p><h2><span id="深度学习概念解析">深度学习概念解析</span></h2><h3><span id="什么是深度学习">什么是深度学习？</span></h3><p>这篇文章将要回答什么是深度学习这个问题．</p><p>整个系列课程会涵盖深度学习领域的众多主题，我们会用很多篇文章来充分解释这些课题，以及它们的应用领域和技术实现。</p><p>在这个深度学习系列中所有的文章按特定顺序发表，一些文章中某些概念是基于之前讨论过的概念之上，因此如果你还不熟悉我们正在使用的某些术语或示例，请务必查看先前的文章。</p><p>让我们给出深度学习的定义。</p><blockquote><p>深度学习是机器学习的一个子领域，它所使用的算法是由人类大脑神经网络结构和功能所启发。</p></blockquote><p>我们现在所讲的深度学习，同样采用从数据中学习的算法，就像我们在上一篇关于机器学习的文章中所讨论的那样。 然而，这里的学习算法或模型是基于大脑神经网络的结构和功能。</p><p>我们在深度学习中使用的神经网络不是真正的生物神经网络。 他们只是与生物神经网络共享一些特征，因此，我们称之为人工神经网络（ANNs）。</p><p>我们也经常使用其他术语来指代人工神经网络。 在深度学习领域，人工神经网络（ANN）这个术语有时候也用下面的一些表达：</p><ul><li>网络(net)</li><li>神经网络(neural net)</li><li>模型(model)</li></ul><p><img src="http://plgrq7ioi.bkt.clouddn.com/layers.png" alt="a simple artificial neural network or ANN"></p><h3><span id="深度学习中的深度指的是什么">深度学习中的＂深度＂指的是什么？</span></h3><p>要理解深度学习中的＂深度＂指的是什么，我们首先需要了解人工神经网络的结构。 我们会发现，深度学习使用了一种我们称之为深网(deep net)或深层人工神经网络(deep artificial neural network)的特殊人工神经网络。</p><p>在下一篇关于人工神经网络的文章中，我们将学习如何构建人工神经网络，这将为我们提供理解人工神经网络如何成为深度人工神经网络所需的知识。</p><p>现在，你需要了解以下内容：</p><ol><li>ANN是使用我们称之为神经元的方法构建的</li><li>人工神经网络中的神经元通过层(layer)的方法构建。</li><li>ANN中除了输入层(input layer)和输出层(output layer)之外的所有层都称为隐藏层(hidden layers)。</li><li>如果ANN具有多个隐藏层，则称该ANN为深度ANN</li></ol><p><img src="http://plgrq7ioi.bkt.clouddn.com/layers4.png" alt="deep neural network with 4 layers"></p><p>总结一下，深度学习使用具有多个隐藏层的ANN结构。 在学习过程中请牢记这一点，随着我们对深度学习理解的加深，我们会越来越明白这一点。 期待下个文章跟你相遇！</p><span class="highlight-text purple">公众微信：友邻学社关注获取更多内容</span><p><img src="http://plgrq7ioi.bkt.clouddn.com/qrcode.jpg" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>什么是机器学习？</title>
      <link href="/2019/01/16/No%201%20%E4%BB%80%E4%B9%88%E6%98%AF%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/01/16/No%201%20%E4%BB%80%E4%B9%88%E6%98%AF%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p>机器学习与深度学习基础教程－从零开始</p><p>本系列教程是为初学者定制，涵盖并解释了深度学习和人工神经网络的基本概念</p><a id="more"></a><p><img src="http://plgrq7ioi.bkt.clouddn.com/cover_1.jpg" alt="cover_1"></p><!-- toc --><ul><li><a href="#什么是机器学习">什么是机器学习？</a><ul><li><a href="#初识深度学习与人工神经网络">初识深度学习与人工神经网络</a></li><li><a href="#什么是机器学习-1">什么是机器学习？</a></li><li><a href="#机器学习-vs-传统编程">机器学习　v.s.　传统编程</a></li><li><a href="#更进一步深度学习">更进一步：深度学习</a></li></ul></li></ul><!-- tocstop --><p><img src="http://plgrq7ioi.bkt.clouddn.com/image_1.png" alt="image_1"></p><h2><span id="什么是机器学习">什么是机器学习？</span></h2><h3><span id="初识深度学习与人工神经网络">初识深度学习与人工神经网络</span></h3><p>本系列教程是为初学者定制，涵盖并解释了深度学习和人工神经网络的基本概念。</p><p>除了讲清基本概念，我们还展示了如何使用Keras（一种用Python编写的神经网络API）在代码中实现这些概念。 我们将学习人工神经网络中的层(layer)的概念，以及激活函数(activation function)，反向传播(backpropagation)，卷积神经网络（CNN），数据增强(data augmentation)，转移学习等概念。</p><p>我们通过讨论不同的术语，认识并理解它们的含义，搞清楚它们是如何适应整个深度学习框架的。 在某些章节里，我们会教大家如何在代码中实现一些课题。</p><p>我们会用Python编写的Keras神经网络API来进行演示，展示这些基本课题的技术实现。</p><p><img src="/home/scott/Documents/blog/source/_posts/keras%20logo%20with%20text.png" alt="keras logo"></p><p>本网站有一个单独的系列专门介绍如何使用Keras，给大家作为重要参考。 Keras系列中的许多内容都假定你已经对本系列中将要讨论的深度学习知识有了基本的了解，请先浏览这个基础教程。</p><p>如果你想研究我们在本系列中所做的代码实现，可以转到Keras系列教程中了解Keras的预备知识，掌握这些先决知识，以便更好地使用Keras。</p><h3><span id="什么是机器学习">什么是机器学习？</span></h3><p>让我们从最基础的讲起。</p><p>深度学习是我们课程的核心概念，我们需要从基础着手更深入地认识它的含义。 在了解深度学习之前，我们首先要了解什么是机器学习。</p><blockquote><p>机器学习是使用算法分析数据，从该数据中学习，然后对新数据进行确定或预测的实践。</p></blockquote><p>这个概念听起来好像跟我们平常用写的代码差不多，似乎也是下面的路数：</p><ol><li>设计一个算法</li><li>机器在一个特定的数据集上执行这个算法</li><li>然后机器就可以在从未见过的新数据上再次执行这个任务</li></ol><p>都是给计算机下指令，那机器学习和我们平常写的逻辑算法区别在哪里呢？</p><p>根据我们刚刚给出的机器学习定义，重心是在＂从数据中学习＂这个部分．</p><blockquote><p>learn from data</p></blockquote><p>通过机器学习，而不是手动编写具有特定指令集的代码来完成特定任务，机器使用数据和算法进行训练，使其能够自主执行任务而无需明确告知如何执行任务。</p><p>现在这可能听起来像天方夜谭，但我们会在后面的文章中看到它是如何一步步实现的。 为了区别使用机器学习和传统编程来完成一项工作，我们看下面的示例。</p><p><img src="http://plgrq7ioi.bkt.clouddn.com/robot.jpg" alt="toy robot with a winder on its head and gears for ears (/home/scott/Documents/blog/source/_posts/robot%20machine.jpg)"></p><h3><span id="机器学习-vs-传统编程">机器学习　v.s.　传统编程</span></h3><p>示例：分析一个大众社交平台的评论，把这些评论分类为正面或负面情绪。</p><p><strong>传统的编程方法</strong><br>该算法可以首先查找与负面或正面情绪相关联的特定单词。</p><p>使用条件语句，算法会基于所查找到的正面或负面单词数目比例，将文章分类为正面或负面评价。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//伪代码</span></span><br><span class="line">let positive = [</span><br><span class="line">    <span class="string">"happy"</span>, </span><br><span class="line">    <span class="string">"thankful"</span>, </span><br><span class="line">    <span class="string">"amazing"</span></span><br><span class="line">];</span><br><span class="line"></span><br><span class="line">let negative = [</span><br><span class="line">    <span class="string">"can't"</span>, </span><br><span class="line">    <span class="string">"won't"</span>, </span><br><span class="line">    <span class="string">"sorry"</span>, </span><br><span class="line">    <span class="string">"unfortunately"</span></span><br><span class="line">];</span><br></pre></td></tr></table></figure><p>这些词汇的选择是程序员任意指定的。 一旦我们得到正面和负面单词的列表，一个简单的算法就是简单地计算给定文章中每种类型单词的出现次数。 然后，基于正例或负例那种次数更多，可以将文章分类为正向或负向。</p><p><strong>机器学习方法</strong><br>该算法分析给定的媒体数据并学习能够将负面文章与正面文章分类的特征。</p><p>通过学到的知识，算法可以将新文章分类为正面或负面。 在这种情况下，设计机器学习程序不需要明确指定需要算法识别的单词。 相反，该算法将通过检验每篇文章的标签来“学习”某些单词是正面还是负面。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//伪代码</span></span><br><span class="line">let articles = [</span><br><span class="line">    &#123;</span><br><span class="line">        label: <span class="string">"positive"</span>,</span><br><span class="line">        data: <span class="string">"The lizard movie was great! I really liked..."</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        label: <span class="string">"positive"</span>,</span><br><span class="line">        data: <span class="string">"Awesome lizards! The color green is my fav..."</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        label: <span class="string">"negative"</span>,</span><br><span class="line">        data: <span class="string">"Total disaster! I never liked..."</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        label: <span class="string">"negative"</span>,</span><br><span class="line">        data: <span class="string">"Worst movie of all time!..."</span></span><br><span class="line">    &#125;</span><br><span class="line">];</span><br></pre></td></tr></table></figure><h3><span id="更进一步深度学习">更进一步：深度学习</span></h3><p>在我们大概了解机器学习之后，就能够在此基础上认识深度学习。 深度学习是一种可用于实现机器学习的工具或技术。 下一节我们将详细介绍深入学习，敬请期待。 希望我们的文章可以帮到你！</p><span class="highlight-text purple">公众微信：友邻学社关注获取更多内容</span><p><img src="http://plgrq7ioi.bkt.clouddn.com/qrcode.jpg" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
