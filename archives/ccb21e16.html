
<!DOCTYPE html>
<html lang="zh-cn,en,ja,default">
    
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="机器学习">
    <title>SoPa: Bridging CNNs, RNNs, and Weighted Finite-State Machines - 机器学习</title>
    <meta name="author" content="天津亦友拾光文化传播有限公司">
    
        <meta name="keywords" content="deep learning,neural network,mechine learning,ANN,AI,深度学习,机器学习,">
    
    
        <link rel="icon" href="http://cdn.yiyouls.com/favicon.ico">
    
    
        <link rel="alternate" type="application/atom+xml" title="RSS" href="/atom.xml">
    
    <script type="application/ld+json">{"@context":"http://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"天津亦友拾光文化传播有限公司","sameAs":["https://github.com/yiyouls","http://stackoverflow.com/users/10926004/yiyouls","https://twitter.com/hanhaishiyi","https://facebook.com/yiyouls","https://plus.google.com/u/0/108244569432480563366","https://www.linkedin.com/in/yiyouls","https://www.zhihu.com/people/yiyouls-66/activities","https://study.163.com/provider/400000000637008/index.htm?share=2&shareId=400000000637008","http://space.bilibili.com/275230151?","hanhaishiyi@gmail.com"],"image":"http://cdn.yiyouls.com/profile.png"},"articleBody":"AbstractRecurrent and convolutional neural networks comprise two distinct families of models that have proven to be useful for encoding natural language utterances.\nIn this paper we present SoPa, a new model that aims to bridge these two approaches.\nSoPa combines neural representation learning with weighted finite-state\nautomata (WFSAs) to learn a soft version of traditional surface patterns.\nWe show that SoPa is an extension of a one-layer CNN, and that such CNNs are equivalent to a restricted version of SoPa, and accordingly, to a restricted form of WFSA.\nEmpirically, on three text classification tasks, SoPa is comparable or better than both a BiLSTM (RNN) baseline and a CNN baseline, and is particularly useful in small data settings.\n\n\n\n1 Introduction\n2 Background\nSurface patterns.\nWFSAs.\n\n\n3 SoPa: A Weighted Finite-State Automaton RNN\n3.1 Patterns as WFSAs\nWords vs. wildcards.\n3.2 Scoring Documents\nImplementation details.\n\n\n3.3 Aggregating Multiple Patterns\n3.4 SoPa as an RNN SoPa can be considered an RNN.\n4 SoPa as a CNN Extension\n5 Experiments\nDatasets.\nReduced training data.\nBaselines.\nNumber of patterns.\nAblation analysis.\nInterpreting a single pattern.\nInterpreting a document.\n\n\n\n\nRelated Work\nRecurrent neural networks.\nConvolutional neural networks.\nNeural networks and patterns.\n\n\nConclusion\n\n\n1 IntroductionRecurrent neural networks (RNNs; Elman, 1990) and convolutional neural networks (CNNs; LeCun, 1998) are two of the most useful text representation learners in NLP (Goldberg, 2016).\nThese methods are generally considered to be quite different: the former encodes an arbitrarily long sequence of text, and is highly expressive (Siegelmann and Sontag, 1995).\nThe latter is more local, encoding fixed length windows, and accordingly less expressive.\nIn this paper, we seek to bridge the gap between RNNs and CNNs, presenting SoPa (for Soft Patterns), a model that lies in between them.\nSoPa is a neural version of a weighted finite-state automaton (WFSA), with a restricted set of transitions.\nLinguistically, SoPa is appealing as it is able to capture a soft notion of surface patterns (e.g., “what a great X !”; Hearst, 1992), where some words may be dropped, inserted, or replaced with similar words.\nFrom a modeling perspective, SoPa is interesting because WFSAs are well-studied and come with efficient and flexible inference algorithms (Mohri, 1997; Eisner, 2002) that SoPa can take advantage of.\nSoPa defines a set of soft patterns of different lengths, with each pattern represented as a WFSA .\nWhile the number and lengths of the patterns are hyperparameters, the patterns themselves are learned end-to-end.\nSoPa then represents a document with a vector that is the aggregate of the scores computed by matching each of the patterns with each span in the document.\nBecause SoPa defines a hidden state that depends on the input token and the previous state, it can be thought of as a simple type of RNN.\nWe show that SoPa is an extension of a one-layer CNN.\nAccordingly, one-layer CNNs can be viewed as a collection of linear-chain WFSAs, each of which can only match fixed-length spans, while our extension allows matches of flexible-length.\nAs a simple type of RNN that is more expressive than a CNN, SoPa helps to link CNNs and RNNs.\nTo test the utility of SoPa, we experiment with three text classification tasks .\nWe compare against four baselines, including both a bidirectional LSTM and a CNN.\nOur model performs on par with or better than all baselines on all tasks.\nMoreover, when training with smaller datasets, SoPa is particularly useful, outperforming all models by substantial margins.\nFinally, building on the connections discovered in this paper, we offer a new, simple method to interpret SoPa .\nThis method applies equally well to CNNs.\nWe release our code at https://github.com/NoahsARK/soft_patterns.\n2 BackgroundSurface patterns.Patterns (Hearst, 1992) are particularly useful tool in NLP (Lin et al., 2003; Etzioni et al., 2005; Schwartz et al., 2015).\nThe most basic definition of a pattern is a sequence of words and wildcards (e.g., “X is a Y”), which can either be manually defined or extracted from a corpus using cooccurrence statistics.\nPatterns can then be matched against a specific text span by replacing wildcards with concrete words.\nDavidov et al.\n(2010) introduced a flexible notion of patterns, which supports partial matching of the pattern with a given text by skipping some of the words in the pattern, or introducing new words.\nIn their framework, when a sequence of text partially matches a pattern, hard-coded partial scores are assigned to the pattern match.\nHere, we represent patterns as WFSAs with neural weights, and support these partial matches in a soft manner.\nWFSAs.We review weighted finite-state automata with \u000f-transitions before we move on to our special case in Section 3.\nA WFSA with d states over a vocabulary V is formally defined as a tuple F = hπ, T, ηi, where π ∈ R d is an initial weight vector, T : (V ∪ {\u000f}) → R d×d is a transition weight function, and η ∈ R d is a final weight vector.\nGiven a sequence of words in the vocabulary x = hx1, .\n, xni, the Forward algorithm (Baum and Petrie, 1966) scores x with respect to F. Without \u000f-transitions, Forward can be written as a series of matrix multiplications: p 0 span(x) = π &gt; Yn i=1 T(xi) !\nη \u000f-transitions are followed without consuming a word, so Equation 1 must be updated to reflect the possibility of following any number (zero or more) of \u000f-transitions in between consuming each word:\npspan(x) = π &gt;T(\u000f) ∗ Yn i=1 T(xi)T(\u000f) ∗ ! η (2)\nwhere ∗ is matrix asteration: A∗ := P∞ j=0 Aj .\nIn our experiments we use a first-order approximation, A∗ ≈ I + A, which corresponds to allowing zero or one \u000f-transition at a time.\nWhen the FSA F is probabilistic, the result of the Forward algorithm can be interpreted as the marginal probability of all paths through F while consuming x (hence the symbol “p”).\nThe Forward algorithm can be generalized to any semiring (Eisner, 2002), a fact that we make use of in our experiments and analysis.1 The vanilla version of Forward uses the sum-product semiring: ⊕ is addition, ⊗ is multiplication.\nA special case of Forward is the Viterbi algorithm (Viterbi, 1967), which sets ⊕ to the max operator.\nViterbi finds the highest scoring path through F while consuming x.\nBoth Forward and Viterbi have runtime O(d 3 + d 2n), requiring just a single linear pass through the phrase.\nUsing firstorder approximate asteration, this runtime drops to O(d 2n).\n2 Finally, we note that Forward scores are for exact matches—the entire phrase must be consumed.\nWe show in Section 3.2 how phrase-level scores can be summarized into a document-level score.\n3 SoPa: A Weighted Finite-State Automaton RNNWe introduce SoPa, a WFSA-based RNN, which is designed to represent text as collection of surface pattern occurrences.\nWe start by showing how a single pattern can be represented as a WFSA-\u000f (Section 3.1).\nThen we describe how to score a complete document using a pattern (Section 3.2), and how multiple patterns can be used to encode a document (Section 3.3).\nFinally, we show that SoPa can be seen as a simple variant of an RNN (Section 3.4)\n3.1 Patterns as WFSAsWe describe how a pattern can be represented as a WFSA-\u000f.\nWe first assume a single pattern.\nA pattern is a WFSA-\u000f, but we impose hard constraints on its shape, and its transition weights are given by differentiable functions that have the power to capture concrete words, wildcards, and everything in between.\nOur model is designed to behave similarly to flexible hard patterns (see Section 2), but to be learnable directly and “end-to-end” through backpropagation.\nImportantly, it will still be interpretable as simple, almost linear-chain, WFSA-\u000f.\nEach pattern has a sequence of d states (in our experiments we use patterns of varying lengths between 2 and 7).\nEach state i has exactly three possible outgoing transitions: a self-loop, which allows the pattern to consume a word without moving states, a main path transition to state i + 1 which allows the pattern to consume one token and move forward one state, and an \u000f-transition to state i + 1, which allows the pattern to move forward one state without consuming a token.\nAll other transitions are given score 0.\nWhen processing a sequence of text with a pattern p, we start with a special START state, and only move forward (or stay put), until we reach the special END state.3 A pattern with d states will tend to match token spans of length d − 1 (but possibly shorter spans due to \u000f-transitions, or longer spans due to self-loops).\nSee Figure 1 for an illustration\nOur transition function, T, is a parameterized function that returns a d × d matrix.\nFor a word x: [T(x)]i,j =    E(ui · vx + ai), if j = i (self-loop) E(wi · vx + bi), if j = i + 1 0, otherwise, (3) where ui and wi are vectors of parameters, ai and bi are scalar parameters, vx is a fixed pre-trained word vector for x, 4 and E is an encoding function, typically the identity function or sigmoid.\n\u000f-transitions are also parameterized, but don’t consume a token and depend only on the current state: [T(\u000f)]i,j = ( E(ci), if j = i + 1 0, otherwise, (4) where ci is a scalar parameter.5\nAs we have only three non-zero diagonals in total, the matrix multiplications in Equation 2 can be implemented using vector operations, and the overall runtimes of Forward and Viterbi are reduced to O(dn). 6\nWords vs. wildcards.Traditional hard patterns distinguish between words and wildcards.\nOur model does not explicitly capture the notion of either, but the transition weight function can be interpreted in those terms.\nEach transition is a logistic regression over the next word vector vx.\nFor example, for a main path out of state i, T has two parameters, wi and bi .\nIf wi has large magnitude and is close to the word vector for some word y (e.g., wi ≈ 100vy), and bi is a large negative bias (e.g., bi ≈ −100), then the transition is essentially matching the specific word y.\nWhereas if wi has small magnitude (wi ≈ 0) and bi is a large positive bias (e.g., bi ≈ 100), then the transition is ignoring the current token and matching a wildcard.7 The transition could also be something in between, for instance by focusing on specific dimensions of a word’s meaning encoded in the vector, such as POS or semantic features like animacy or concreteness (Rubinstein et al., 2015; Tsvetkov et al., 2015).\n3.2 Scoring DocumentsSo far we described how to calculate how well a pattern matches a token span exactly (consuming the whole span).\nTo score a complete document, we prefer a score that aggregates over all matches on subspans of the document (similar to “search” instead of “match” in regular expression parlance).\nWe still assume a single pattern.\nEither the Forward algorithm can be used to calculate the expected count of the pattern in the document, P 1≤i≤j≤n pspan(xi:j ), or Viterbi to calculate sdoc(x) = max1≤i≤j≤n sspan(xi:j ), the score of the highest-scoring match.\nIn short documents, we expect patterns to typically occur at most once, so in our experiments we choose the Viterbi algorithm, i.e., the max-product semiring.\nImplementation details.We give the specific recurrences we use to score documents in a single pass with this model. We define:\n[maxmul(A, B)]i,j = max k Ai,kBk,j .\n(5) We also define the following for taking zero or one \u000f-transitions: eps (h) = maxmul (h, max(I, T(\u000f))) (6) where max is element-wise max.\nWe maintain a row vector ht at each token:8 h0 = eps(π &gt;), (7a) ht+1 = max (eps(maxmul (ht , T(xt+1))), h0), (7b) and then extract and aggregate END state values: st = maxmul (ht , η), (8a) sdoc = max 1≤t≤n st .\n(8b) [ht ]i represents the score of the best path through the pattern that ends in state i after consuming t tokens.\nBy including h0 in Equation 7b, we are accounting for spans that start at time t + 1. st is the maximum of the exact match scores for all spans ending at token t. And sdoc is the maximum score of any subspan in the document.\n3.3 Aggregating Multiple PatternsWe describe how k patterns are aggregated to score a document.\nThese k patterns give k different sdoc scores for the document, which are stacked into a vector z ∈ R k and constitute the final document representation of SoPa.\nThis vector representation can be viewed as a feature vector.\nIn this paper, we feed it into a multilayer perceptron (MLP), culminating in a softmax to give a probability distribution over document labels.\nWe minimize cross-entropy, allowing the SoPa and MLP parameters to be learned end-to-end.\nSoPa uses a total of (2e + 3)dk parameters, where e is the word embedding dimension, d is the number of states and k is the number of patterns.\nFor comparison, an LSTM with a hidden dimension of h has 4((e + 1)h + h 2 ).\nIn Section 6 we show that SoPa consistently uses fewer parameters than a BiLSTM baseline to achieve its best result.\n3.4 SoPa as an RNN SoPa can be considered an RNN.As shown in Section 3.2, a single pattern with d states has a hidden state vector of size d. Stacking the k hidden state vectors of k patterns into one vector of size k × d can be thought of as the hidden state of our model.\nThis hidden state is, like in any other RNN, dependent of the input and the previous state.\nUsing selfloops, the hidden state at time point i can in theory depend on the entire history of tokens up to xi (see Figure 2 for illustration).\nWe do want to discourage the model from following too many self-loops, only doing so if it results in a better fit with the remainder of the pattern.\nTo do this we use the sigmoid function as our encoding function E (see Equation 3), which means that all transitions have scores strictly less than 1.\nThis works to keep pattern matches close to their intended length.\nUsing other encoders, such as the identity function, can result in different dynamics, potentially encouraging rather than discouraging self-loops.\nAlthough even single-layer RNNs are Turing complete (Siegelmann and Sontag, 1995), SoPa’s expressive power depends on the semiring.\nWhen a WFSA is thought of as a function from finite sequences of tokens to semiring values, it is restricted to the class of functions known as rational series (Schutzenberger ¨ , 1961; Droste and Gastin, 1999; Sakarovitch, 2009).9 It is unclear how limiting this theoretical restriction is in practice, especially when SoPa is used as a component in a larger network.\nWe defer the investigation of the exact computational properties of SoPa to future work.\nIn the next section, we show that SoPa is an extension of a one-layer CNN, and hence more expressive.\n4 SoPa as a CNN ExtensionA convolutional neural network (CNN; LeCun, 1998) moves a fixed-size sliding window over the document, producing a vector representation for each window.\nThese representations are then often summed, averaged, or max-pooled to produce a document-level representation (Kim, 2014; Yin and Schutze ¨ , 2015).\nIn this section, we show that SoPa is an extension of one-layer, max-pooled CNNs.\n\nFigure 2: State activations of two patterns as they score a document. pattern1 (length three) matches on “in years”. pattern2 (length five) matches on “funniest and most likeable book”, using a self-loop to consume the token “most”. Active states in the best match are marked with arrow cursors.\n\nTo recover a CNN from a soft pattern with d+ 1 states, we first remove self-loops and \u000f-transitions, retaining only the main path transitions.\nWe also use the identity function as our encoder E (Equation 3), and use the max-sum semiring.\nWith only main path transitions, the network will not match any span that is not exactly d tokens long.\nUsing max-sum, spans of length d will be assigned the score: sspan(xi:i+d) =X d−1 j=0 wj · vxi+j + bj , (9a) =w0:d · vxi:i+d + X d−1 j=0 bj , (9b) where w0:d = [w&gt; 0 ; .\n; w&gt; d−1 ] &gt;, vxi:i+d = [v &gt; xi ; .\n; v &gt; xi+d−1 ] &gt;.\nRearranged this way, we recognize the span score as an affine transformation of the concatenated word vectors vxi:i+d .\nIf we use k patterns, then together their span scores correspond to a linear filter with window size d and output dimension k. 10 A single pattern’s score for a document is: sdoc(x) = max 1≤i≤n−d+1 sspan(xi:i+d).\n(10) The max in Equation 10 is calculated for each pattern independently, corresponding exactly to element-wise max-pooling of the CNN’s output layer.\nBased on the equivalence between this impoverished version of SoPa and CNNs, we conclude that one-layer CNNs are learning an even more 10This variant of SoPa has d bias parameters, which correspond to only a single bias parameter in a CNN.\nThe redundant biases may affect optimization but are an otherwise unimportant difference.\nrestricted class of WFSAs (linear-chain WFSAs) that capture only fixed-length patterns.\nOne notable difference between SoPa and arbitrary CNNs is that in general CNNs can use any filter (like an MLP over vxi:i+d , for example).\nIn contrast, in order to efficiently pool over flexiblelength spans, SoPa is restricted to operations that follow the semiring laws.11 As a model that is more flexible than a one-layer CNN, but (arguably) less expressive than many RNNs, SoPa lies somewhere on the continuum between these two approaches.\nContinuing to study the bridge between CNNs and RNNs is an exciting direction for future research.\n5 ExperimentsTo evaluate SoPa, we apply it to text classification tasks.\nBelow we describe our datasets and baselines.\nMore details can be found in Appendix A. \nDatasets.We experiment with three binary classification datasets.\n• SST.\nThe Stanford Sentiment Treebank (Socher et al., 2013) 12 contains roughly 10K movie reviews from Rotten Tomatoes,13 labeled on a scale of 1–5.\nWe consider the binary task, which considers 1 and 2 as negative, and 4 and 5 as positive (ignoring 3s).\nIt is worth noting that this dataset also contains syntactic phrase level annotations, providing a sentiment label to parts of   sentences.\nIn order to experiment in a realistic setup, we only consider the complete sentences, and ignore syntactic annotations at train or test time.\nThe number of training/development/test sentences in the dataset is 6,920/872/1,821.\n• Amazon.\nThe Amazon Review Corpus (McAuley and Leskovec, 2013) 14 contains electronics product reviews, a subset of a larger review dataset.\nEach document in the dataset contains a review and a summary.\nFollowing Yogatama et al.\n(2015), we only use the reviews part, focusing on positive and negative reviews.\nThe number of training/development/test samples is 20K/5K/25K.\n• ROC.\nThe ROC story cloze task (Mostafazadeh et al., 2016) is a story understanding task.15 The task is composed of four-sentence story prefixes, followed by two competing endings: one that makes the joint five-sentence story coherent, and another that makes it incoherent.\nFollowing Schwartz et al.\n(2017), we treat it as a style detection task: we treat all “right” endings as positive samples and all “wrong” ones as negative, and we ignore the story prefix.\nWe split the development set into train and development (of sizes 3,366 and 374 sentences, respectively), and take the test set as-is (3,742 sentences).\nReduced training data.In order to test our model’s ability to learn from small datasets, we also randomly sample 100, 500, 1,000 and 2,500 SST training instances and 100, 500, 1,000, 2,500, 5,000, and 10,000 Amazon training instances.\nDevelopment and test sets remain the same.\nBaselines.We compare to four baselines: a BiLSTM, a one-layer CNN, DAN (a simple alternative to RNNs) and a feature-based classifier trained with hard-pattern features.\n• BiLSTM.\nBidirectional LSTMs have been successfully used in the past for text classification tasks (Zhou et al., 2016).\nWe learn a one-layer BiLSTM representation of the document, and feed the average of all hidden states to an MLP.\n• CNN.\nCNNs are particularly useful for text classification (Kim, 2014).\nWe train a one-layer CNN with max-pooling, and feed the resulting representation to an MLP.\n• DAN.\nWe learn a deep averaging network with word dropout (Iyyer et al., 2015), a simple but strong text-classification baseline.\n• Hard.\nWe train a logistic regression classifier with hard-pattern features.\nFollowing Tsur et al.\n(2010), we replace low frequency words with a special wildcard symbol.\nWe learn sequences of 1–6 concrete words, where any number of wildcards can come between two adjacent words.\nWe consider words occurring with frequency of at least 0.01% of our training set as concrete words, and words occurring in frequency 1% or less as wildcards.1\nNumber of patterns.SoPa requires specifying the number of patterns to be learned, and their lengths.\nPreliminary experiments showed that the model doesn’t benefit from more than a few dozen patterns.\nWe experiment with several configurations of patterns of different lengths, generally considering 0, 10 or 20 patterns of each pattern length between 2–7.\nThe total number of patterns learned ranges between 30–70.17 6 Results Table 1 shows our main experimental results.\nIn two of the cases (SST and ROC), SoPa outperforms all models.\nOn Amazon, SoPa performs within 0.3 points of CNN and BiLSTM, and outperforms the other two baselines.\nThe table also shows the number of parameters used by each model for each task.\nGiven enough data, models with more parameters should be expected to perform better.\nHowever, SoPa performs better or roughly the same as a BiLSTM, which has 3–6 times as many parameters.\nFigure 3 shows a comparison of all models on the SST and Amazon datasets with varying training set sizes.\nSoPa is substantially outperforming all baselines, in particular BiLSTM, on small datasets (100 samples).\nThis suggests that SoPa is better fit to learn from small datasets.\nAblation analysis.Table 1 also shows an ablation of the differences between SoPa and CNN: max-product semiring with sigmoid vs. max-sum semiring with identity, self-loops, and \u000f-transitions.\nThe last line is equivalent to a CNN with multiple window sizes.\nInterestingly, the most notable difference between SoPa and CNN is the semiring and encoder function, while \u000f transitions and self-loops have little effect on performance.18 7 Interpretability We turn to another key aspect of SoPa—its interpretability.\nWe start by demonstrating how we interpret a single pattern, and then describe how to interpret the decisions made by downstream classifiers that rely on SoPa—in this case, a sentence classifier.\nImportantly, these visualization techniques are equally applicable to CNNs.\nInterpreting a single pattern.In order to visualize a pattern, we compute the pattern matching scores with each phrase in our training dataset, and select the k phrases with the highest scores.\nTable 2 shows examples of six patterns learned using the best SoPa model on the SST dataset, as represented by their five highest scoring phrases in the training set.\nA few interesting trends can be observed from these examples.\nFirst, it seems our patterns encode semantically coherent expressions.\nA large portion of them correspond to sentiment (the five top examples in the table), but others capture different semantics, e.g., time expressions.\nSecond, it seems our patterns are relatively soft, and allow lexical flexibility.\nWhile some patterns do seem to fix specific words, e.g., “of” in the first example or “minutes” in the last one, even in those cases some of the top matching spans replace these words with other, similar words (“with” and “halfhour”, respectively).\nEncouraging SoPa to have more concrete words, e.g., by jointly learning the word vectors, might make SoPa useful in other contexts, particularly as a decoder.\nWe defer this direction to future work.\nFinally, SoPa makes limited but non-negligible use of self-loops and epsilon steps.\nInterestingly, the second example shows that one of the patterns had an \u000f-transition at the same place in every phrase.\nThis demonstrates a different function of \u000f-transitions than originally designed—they allow a pattern to effectively shorten itself, by learning a high \u000f-transition parameter for a certain state.\nInterpreting a document.SoPa provides an interpretable representation of a document—a vector of the maximal matching score of each pattern with any span in the document.\nTo visualize the decisions of our model for a given document, we can observe the patterns and corresponding phrases that score highly within it.\nTo understand which of the k patterns contributes most to the classification decision, we apply a leave-one-out method.\nWe run the forward method of the MLP layer in SoPa k times, each time zeroing-out the score of a different pattern p. The difference between the resulting score and the original model score is considered p’s contribution.\nWe then consider the highest contributing patterns, and attach each one with its highest scoring phrase in that document.\nTable 3 shows example texts along with their most positive and negative contributing phrases.\nRelated WorkWeighted finite-state automata.\nWFSAs and hidden Markov models19 were once popular in automatic speech recognition (Hetherington, 2004; Moore et al., 2006; Hoffmeister et al., 2012) and remain popular in morphology (Dreyer, 2011; Cotterell et al., 2015).\nMost closely related to this work, neural networks have been combined with weighted finite-state transducers to do morphological reinflection (Rastogi et al., 2016).\nThese prior works learn a single FSA or FST, whereas our model learns a collection of simple but complementary FSAs, together encoding a sequence.\nWe are the first to incorporate neural networks both before WFSAs (in their transition scoring functions), and after (in the function that turns their vector of scores into a final prediction), to produce an expressive model that remains interpretable.\nRecurrent neural networks.The ability of RNNs to represent arbitrarily long sequences of embedded tokens has made them attractive to NLP researchers.\nThe most notable variants, the long short-term memory (LSTM; Hochreiter and Schmidhuber, 1997) and gated recurrent units (GRU; Cho et al., 2014), have become ubiquitous in NLP algorithms (Goldberg, 2016).\nRecently, several works introduced simpler versions of RNNs, such as recurrent additive networks (Lee et al., 2017) and Quasi-RNNs (Bradbury et al., 2017).\nLike SoPa, these models can be seen as points along the bridge between RNNs and CNNs.\nOther works have studied the expressive power of RNNs, in particular in the context of WFSAs or HMMs (Cleeremans et al., 1989; Giles et al., 1992; Visser et al., 2001; Chen et al., 2018).\nIn this work we relate CNNs to WFSAs, showing that a one-layer CNN with max-pooling can be simulated by a collection of linear-chain WFSAs.\nConvolutional neural networks.CNNs are prominent feature extractors in NLP, both for generating character-based embeddings (Kim et al., 2016), and as sentence encoders for tasks like text classification (Yin and Schutze ¨ , 2015) and machine translation (Gehring et al., 2017).\nSimilarly to SoPa, several recently introduced variants of CNNs support varying window sizes by either allowing several fixed window sizes (Yin and Schutze ¨ , 2015) or by supporting non-consecutive n-gram matching (Lei et al., 2015; Nguyen and Grishman, 2016).\nNeural networks and patterns.Some works used patterns as part of a neural network.\nSchwartz et al. (2016) used pattern contexts for estimating word embeddings, showing improved word similarity results compared to bag-of-word contexts.\nShwartz et al. (2016) designed an LSTM representation for dependency patterns, using them to detect hypernymy relations.\nHere, we learn patterns as a neural version of WFSAs.\nInterpretability. There have been several efforts to interpret neural models.\nThe weights of the attention mechanism (Bahdanau et al., 2015) are often used to display the words that are most significant for making a prediction.\nLIME (Ribeiro et al., 2016) is another approach for visualizing neural models (not necessarily textual).\nYogatama and Smith (2014) introduced structured sparsity, which encodes linguistic information into the regularization of a model, thus allowing to visualize the contribution of different bag-of-word features.\nOther works jointly learned to encode text and extract the span which best explains the model’s prediction (Yessenalina et al., 2010; Lei et al., 2016).\nLi et al. (2016) and Kad´ ar et al. ´ (2017) suggested a method that erases pieces of the text in order to analyze their effect on a neural model’s decisions.\nFinally, several works presented method to visualize deep CNNs (Zeiler and Fergus, 2014; Simonyan et al., 2014; Yosinski et al., 2015), focusing on visualizing the different layers of the network, mainly in the context of image and video understanding.\nWe believe these two types of research approaches are complementary: inventing general purpose visualization tools for existing black-box models on the one hand, and on the other, designing models like SoPa that are interpretable by construction.\nConclusionWe introduced SoPa, a novel model that combines neural representation learning with WFSAs.\nWe showed that SoPa is an extension of a one-layer CNN.\nIt naturally models flexible-length spans with insertion and deletion, and it can be easily customized by swapping in different semirings.\nSoPa performs on par with or strictly better than four baselines on three text classification tasks, while requiring fewer parameters than the stronger baselines.\nOn smaller training sets, SoPa outperforms all four baselines.\nAs a simple version of an RNN, which is more expressive than one-layer CNNs, we hope that SoPa will encourage future research on the bridge between these two mechanisms.\nTo facilitate such research, we release our implementation at https://github.com/Noahs-ARK/soft_patterns.\n","dateCreated":"2019-04-08T20:14:52+08:00","dateModified":"2019-04-08T21:58:35+08:00","datePublished":"2019-04-08T20:14:52+08:00","description":"AbstractRecurrent and convolutional neural networks comprise two distinct families of models that have proven to be useful for encoding natural language utterances.\nIn this paper we present SoPa, a new model that aims to bridge these two approaches.\nSoPa combines neural representation learning with weighted finite-state\nautomata (WFSAs) to learn a soft version of traditional surface patterns.\nWe show that SoPa is an extension of a one-layer CNN, and that such CNNs are equivalent to a restricted version of SoPa, and accordingly, to a restricted form of WFSA.\nEmpirically, on three text classification tasks, SoPa is comparable or better than both a BiLSTM (RNN) baseline and a CNN baseline, and is particularly useful in small data settings.","headline":"SoPa: Bridging CNNs, RNNs, and Weighted Finite-State Machines","image":[],"mainEntityOfPage":{"@type":"WebPage","@id":"http://yoursite.com/archives/ccb21e16.html"},"publisher":{"@type":"Organization","name":"天津亦友拾光文化传播有限公司","sameAs":["https://github.com/yiyouls","http://stackoverflow.com/users/10926004/yiyouls","https://twitter.com/hanhaishiyi","https://facebook.com/yiyouls","https://plus.google.com/u/0/108244569432480563366","https://www.linkedin.com/in/yiyouls","https://www.zhihu.com/people/yiyouls-66/activities","https://study.163.com/provider/400000000637008/index.htm?share=2&shareId=400000000637008","http://space.bilibili.com/275230151?","hanhaishiyi@gmail.com"],"image":"http://cdn.yiyouls.com/profile.png","logo":{"@type":"ImageObject","url":"http://cdn.yiyouls.com/profile.png"}},"url":"http://yoursite.com/archives/ccb21e16.html"}</script>
    <meta name="description" content="AbstractRecurrent and convolutional neural networks comprise two distinct families of models that have proven to be useful for encoding natural language utterances. In this paper we present SoPa, a ne">
<meta name="keywords" content="deep learning,neural network,mechine learning,ANN,AI,深度学习,机器学习">
<meta property="og:type" content="blog">
<meta property="og:title" content="SoPa: Bridging CNNs, RNNs, and Weighted Finite-State Machines">
<meta property="og:url" content="http://yoursite.com/archives/ccb21e16.html">
<meta property="og:site_name" content="机器学习">
<meta property="og:description" content="AbstractRecurrent and convolutional neural networks comprise two distinct families of models that have proven to be useful for encoding natural language utterances. In this paper we present SoPa, a ne">
<meta property="og:locale" content="zh-cn">
<meta property="og:updated_time" content="2019-04-08T13:58:35.890Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SoPa: Bridging CNNs, RNNs, and Weighted Finite-State Machines">
<meta name="twitter:description" content="AbstractRecurrent and convolutional neural networks comprise two distinct families of models that have proven to be useful for encoding natural language utterances. In this paper we present SoPa, a ne">
<meta name="twitter:creator" content="@hanhaishiyi">
    
        <link rel="publisher" href="https://plus.google.com/108244569432480563366"/>
    
    
        
    
    
        <meta property="og:image" content="http://cdn.yiyouls.com/profile.png"/>
    
    
    
    
    <!--STYLES-->
    <link rel="stylesheet" href="/assets/css/style-c4ozcsklz4kht2pebhp44xorvyverh23toayhn7i6ubrpyedak24hv1v0hyd.min.css">
    <!--STYLES END-->
    

    
</head>

    <body>
        <div id="blog">
            <!-- Define author's picture -->


    
        
            
        
    

<header id="header" data-behavior="4">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a class="header-title-link" href="/ ">机器学习</a>
    </div>
    
        
            <a class="header-right-picture " href="#about">
        
        
            <img class="header-picture" src="http://cdn.yiyouls.com/profile.png" alt="作者的图片">
        
        </a>
    
</header>

            <!-- Define author's picture -->



        
    

<nav id="sidebar" data-behavior="4">
    <div class="sidebar-container">
        
            <div class="sidebar-profile">
                <a href="/#about">
                    <img class="sidebar-profile-picture" src="http://cdn.yiyouls.com/profile.png" alt="作者的图片">
                </a>
                <h4 class="sidebar-profile-name">天津亦友拾光文化传播有限公司</h4>
                
                    <h5 class="sidebar-profile-bio"><p>浙江大学在读</p>
</h5>
                
            </div>
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="/ " title="首页">
                    
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">首页</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="/all-categories" title="分类">
                    
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">分类</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="/all-tags" title="标签">
                    
                        <i class="sidebar-button-icon fa fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">标签</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="/all-archives" title="归档">
                    
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">归档</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link open-algolia-search" href="#search" title="搜索">
                    
                        <i class="sidebar-button-icon fa fa-search" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">搜索</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="#about" title="关于">
                    
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">关于</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="https://github.com/yiyouls" target="_blank" rel="noopener" title="GitHub">
                    
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="http://stackoverflow.com/users/10926004/yiyouls" target="_blank" rel="noopener" title="Stack Overflow">
                    
                        <i class="sidebar-button-icon fab fa-stack-overflow" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Stack Overflow</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="https://twitter.com/hanhaishiyi" target="_blank" rel="noopener" title="Twitter">
                    
                        <i class="sidebar-button-icon fab fa-twitter" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Twitter</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="https://facebook.com/yiyouls" target="_blank" rel="noopener" title="Facebook">
                    
                        <i class="sidebar-button-icon fab fa-facebook" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Facebook</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="https://plus.google.com/u/0/108244569432480563366" target="_blank" rel="noopener" title="Google Plus">
                    
                        <i class="sidebar-button-icon fab fa-google-plus" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Google Plus</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="https://www.linkedin.com/in/yiyouls" target="_blank" rel="noopener" title="LinkedIn">
                    
                        <i class="sidebar-button-icon fab fa-linkedin" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">LinkedIn</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="https://www.zhihu.com/people/yiyouls-66/activities" target="_blank" rel="noopener" title="知乎">
                    
                        <i class="sidebar-button-icon fa fa-book" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">知乎</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="https://study.163.com/provider/400000000637008/index.htm?share=2&shareId=400000000637008" target="_blank" rel="noopener" title="网易云课堂">
                    
                        <i class="sidebar-button-icon fa fa-school" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">网易云课堂</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="http://space.bilibili.com/275230151?" target="_blank" rel="noopener" title="bilibili">
                    
                        <i class="sidebar-button-icon fa fa-video" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">bilibili</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="/hanhaishiyi@gmail.com" title="邮箱">
                    
                        <i class="sidebar-button-icon fa fa-envelope" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">邮箱</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="/atom.xml" title="RSS">
                    
                        <i class="sidebar-button-icon fa fa-rss" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">RSS</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="4"
                 class="
                        hasCoverMetaIn
                        ">
                
<article class="post">
    
    
        <div class="post-header main-content-wrap text-left">
    
        <h1 class="post-title">
            SoPa: Bridging CNNs, RNNs, and Weighted Finite-State Machines
        </h1>
    
    
        <div class="post-meta">
    <time datetime="2019-04-08T20:14:52+08:00">
	
		    4月 08, 2019
    	
    </time>
    
</div>

    
</div>

    
    <div class="post-content markdown">
        <div class="main-content-wrap">
            <h2><span id="abstract">Abstract</span></h2><p>Recurrent and convolutional neural networks comprise two distinct families of models that have proven to be useful for encoding natural language utterances.</p>
<p>In this paper we present SoPa, a new model that aims to bridge these two approaches.</p>
<p>SoPa combines neural representation learning with weighted finite-state</p>
<p>automata (WFSAs) to learn a soft version of traditional surface patterns.</p>
<p>We show that SoPa is an extension of a one-layer CNN, and that such CNNs are equivalent to a restricted version of SoPa, and accordingly, to a restricted form of WFSA.</p>
<p>Empirically, on three text classification tasks, SoPa is comparable or better than both a BiLSTM (RNN) baseline and a CNN baseline, and is particularly useful in small data settings.</p>
<a id="more"></a>
<h1 id="table-of-contents">目录</h1><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-text">1 Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-text">2 Background</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#undefined"><span class="toc-text">Surface patterns.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#undefined"><span class="toc-text">WFSAs.</span></a></li></ol></li></ol><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-text">3 SoPa: A Weighted Finite-State Automaton RNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-text">3.1 Patterns as WFSAs</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#undefined"><span class="toc-text">Words vs. wildcards.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#undefined"><span class="toc-text">3.2 Scoring Documents</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#undefined"><span class="toc-text">Implementation details.</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-text">3.3 Aggregating Multiple Patterns</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-text">3.4 SoPa as an RNN SoPa can be considered an RNN.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-text">4 SoPa as a CNN Extension</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-text">5 Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#undefined"><span class="toc-text">Datasets.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#undefined"><span class="toc-text">Reduced training data.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#undefined"><span class="toc-text">Baselines.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#undefined"><span class="toc-text">Number of patterns.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#undefined"><span class="toc-text">Ablation analysis.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#undefined"><span class="toc-text">Interpreting a single pattern.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#undefined"><span class="toc-text">Interpreting a document.</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-text">Related Work</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-text">Recurrent neural networks.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-text">Convolutional neural networks.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-text">Neural networks and patterns.</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-text">Conclusion</span></a></li>
<ul>
<li><a href="#1-introduction">1 Introduction</a></li>
<li><a href="#2-background">2 Background</a><ul>
<li><a href="#surface-patterns">Surface patterns.</a></li>
<li><a href="#wfsas">WFSAs.</a></li>
</ul>
</li>
<li><a href="#3-sopa-a-weighted-finite-state-automaton-rnn">3 SoPa: A Weighted Finite-State Automaton RNN</a><ul>
<li><a href="#31-patterns-as-wfsas">3.1 Patterns as WFSAs</a><ul>
<li><a href="#words-vs-wildcards">Words vs. wildcards.</a></li>
<li><a href="#32-scoring-documents">3.2 Scoring Documents</a></li>
<li><a href="#implementation-details">Implementation details.</a></li>
</ul>
</li>
<li><a href="#33-aggregating-multiple-patterns">3.3 Aggregating Multiple Patterns</a></li>
<li><a href="#34-sopa-as-an-rnn-sopa-can-be-considered-an-rnn">3.4 SoPa as an RNN SoPa can be considered an RNN.</a></li>
<li><a href="#4-sopa-as-a-cnn-extension">4 SoPa as a CNN Extension</a></li>
<li><a href="#5-experiments">5 Experiments</a><ul>
<li><a href="#datasets">Datasets.</a></li>
<li><a href="#reduced-training-data">Reduced training data.</a></li>
<li><a href="#baselines">Baselines.</a></li>
<li><a href="#number-of-patterns">Number of patterns.</a></li>
<li><a href="#ablation-analysis">Ablation analysis.</a></li>
<li><a href="#interpreting-a-single-pattern">Interpreting a single pattern.</a></li>
<li><a href="#interpreting-a-document">Interpreting a document.</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#related-work">Related Work</a><ul>
<li><a href="#recurrent-neural-networks">Recurrent neural networks.</a></li>
<li><a href="#convolutional-neural-networks">Convolutional neural networks.</a></li>
<li><a href="#neural-networks-and-patterns">Neural networks and patterns.</a></li>
</ul>
</li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
<!-- tocstop -->
<h2><span id="1-introduction">1 Introduction</span></h2><p>Recurrent neural networks (RNNs; Elman, 1990) and convolutional neural networks (CNNs; LeCun, 1998) are two of the most useful text representation learners in NLP (Goldberg, 2016).</p>
<p>These methods are generally considered to be quite different: the former encodes an arbitrarily long sequence of text, and is highly expressive (Siegelmann and Sontag, 1995).</p>
<p>The latter is more local, encoding fixed length windows, and accordingly less expressive.</p>
<p>In this paper, we seek to bridge the gap between RNNs and CNNs, presenting SoPa (for Soft Patterns), a model that lies in between them.</p>
<p>SoPa is a neural version of a weighted finite-state automaton (WFSA), with a restricted set of transitions.</p>
<p>Linguistically, SoPa is appealing as it is able to capture a soft notion of surface patterns (e.g., “what a great X !”; Hearst, 1992), where some words may be dropped, inserted, or replaced with similar words.</p>
<p>From a modeling perspective, SoPa is interesting because WFSAs are well-studied and come with efficient and flexible inference algorithms (Mohri, 1997; Eisner, 2002) that SoPa can take advantage of.</p>
<p>SoPa defines a set of soft patterns of different lengths, with each pattern represented as a WFSA .</p>
<p>While the number and lengths of the patterns are hyperparameters, the patterns themselves are learned end-to-end.</p>
<p>SoPa then represents a document with a vector that is the aggregate of the scores computed by matching each of the patterns with each span in the document.</p>
<p>Because SoPa defines a hidden state that depends on the input token and the previous state, it can be thought of as a simple type of RNN.</p>
<p>We show that SoPa is an extension of a one-layer CNN.</p>
<p>Accordingly, one-layer CNNs can be viewed as a collection of linear-chain WFSAs, each of which can only match fixed-length spans, while our extension allows matches of flexible-length.</p>
<p>As a simple type of RNN that is more expressive than a CNN, SoPa helps to link CNNs and RNNs.</p>
<p>To test the utility of SoPa, we experiment with three text classification tasks .</p>
<p>We compare against four baselines, including both a bidirectional LSTM and a CNN.</p>
<p>Our model performs on par with or better than all baselines on all tasks.</p>
<p>Moreover, when training with smaller datasets, SoPa is particularly useful, outperforming all models by substantial margins.</p>
<p>Finally, building on the connections discovered in this paper, we offer a new, simple method to interpret SoPa .</p>
<p>This method applies equally well to CNNs.</p>
<p>We release our code at <a href="https://github.com/NoahsARK/soft_patterns" target="_blank" rel="noopener">https://github.com/NoahsARK/soft_patterns</a>.</p>
<h2><span id="2-background">2 Background</span></h2><h4><span id="surface-patterns">Surface patterns.</span></h4><p>Patterns (Hearst, 1992) are particularly useful tool in NLP (Lin et al., 2003; Etzioni et al., 2005; Schwartz et al., 2015).</p>
<p>The most basic definition of a pattern is a sequence of words and wildcards (e.g., “X is a Y”), which can either be manually defined or extracted from a corpus using cooccurrence statistics.</p>
<p>Patterns can then be matched against a specific text span by replacing wildcards with concrete words.</p>
<p>Davidov et al.</p>
<p>(2010) introduced a flexible notion of patterns, which supports partial matching of the pattern with a given text by skipping some of the words in the pattern, or introducing new words.</p>
<p>In their framework, when a sequence of text partially matches a pattern, hard-coded partial scores are assigned to the pattern match.</p>
<p>Here, we represent patterns as WFSAs with neural weights, and support these partial matches in a soft manner.</p>
<h4><span id="wfsas">WFSAs.</span></h4><p>We review weighted finite-state automata with -transitions before we move on to our special case in Section 3.</p>
<p>A WFSA with d states over a vocabulary V is formally defined as a tuple F = hπ, T, ηi, where π ∈ R d is an initial weight vector, T : (V ∪ {}) → R d×d is a transition weight function, and η ∈ R d is a final weight vector.</p>
<p>Given a sequence of words in the vocabulary x = hx1, .</p>
<p>, xni, the Forward algorithm (Baum and Petrie, 1966) scores x with respect to F. Without -transitions, Forward can be written as a series of matrix multiplications: p 0 span(x) = π &gt; Yn i=1 T(xi) !</p>
<p>η -transitions are followed without consuming a word, so Equation 1 must be updated to reflect the possibility of following any number (zero or more) of -transitions in between consuming each word:</p>
<p>pspan(x) = π &gt;T() ∗ Yn i=1 T(xi)T() ∗ ! η (2)</p>
<p>where ∗ is matrix asteration: A∗ := P∞ j=0 Aj .</p>
<p>In our experiments we use a first-order approximation, A∗ ≈ I + A, which corresponds to allowing zero or one -transition at a time.</p>
<p>When the FSA F is probabilistic, the result of the Forward algorithm can be interpreted as the marginal probability of all paths through F while consuming x (hence the symbol “p”).</p>
<p>The Forward algorithm can be generalized to any semiring (Eisner, 2002), a fact that we make use of in our experiments and analysis.1 The vanilla version of Forward uses the sum-product semiring: ⊕ is addition, ⊗ is multiplication.</p>
<p>A special case of Forward is the Viterbi algorithm (Viterbi, 1967), which sets ⊕ to the max operator.</p>
<p>Viterbi finds the highest scoring path through F while consuming x.</p>
<p>Both Forward and Viterbi have runtime O(d 3 + d 2n), requiring just a single linear pass through the phrase.</p>
<p>Using firstorder approximate asteration, this runtime drops to O(d 2n).</p>
<p>2 Finally, we note that Forward scores are for exact matches—the entire phrase must be consumed.</p>
<p>We show in Section 3.2 how phrase-level scores can be summarized into a document-level score.</p>
<h2><span id="3-sopa-a-weighted-finite-state-automaton-rnn">3 SoPa: A Weighted Finite-State Automaton RNN</span></h2><p>We introduce SoPa, a WFSA-based RNN, which is designed to represent text as collection of surface pattern occurrences.</p>
<p>We start by showing how a single pattern can be represented as a WFSA- (Section 3.1).</p>
<p>Then we describe how to score a complete document using a pattern (Section 3.2), and how multiple patterns can be used to encode a document (Section 3.3).</p>
<p>Finally, we show that SoPa can be seen as a simple variant of an RNN (Section 3.4)</p>
<h3><span id="31-patterns-as-wfsas">3.1 Patterns as WFSAs</span></h3><p>We describe how a pattern can be represented as a WFSA-.</p>
<p>We first assume a single pattern.</p>
<p>A pattern is a WFSA-, but we impose hard constraints on its shape, and its transition weights are given by differentiable functions that have the power to capture concrete words, wildcards, and everything in between.</p>
<p>Our model is designed to behave similarly to flexible hard patterns (see Section 2), but to be learnable directly and “end-to-end” through backpropagation.</p>
<p>Importantly, it will still be interpretable as simple, almost linear-chain, WFSA-.</p>
<p>Each pattern has a sequence of d states (in our experiments we use patterns of varying lengths between 2 and 7).</p>
<p>Each state i has exactly three possible outgoing transitions: a self-loop, which allows the pattern to consume a word without moving states, a main path transition to state i + 1 which allows the pattern to consume one token and move forward one state, and an -transition to state i + 1, which allows the pattern to move forward one state without consuming a token.</p>
<p>All other transitions are given score 0.</p>
<p>When processing a sequence of text with a pattern p, we start with a special START state, and only move forward (or stay put), until we reach the special END state.3 A pattern with d states will tend to match token spans of length d − 1 (but possibly shorter spans due to -transitions, or longer spans due to self-loops).</p>
<p>See Figure 1 for an illustration</p>
<p>Our transition function, T, is a parameterized function that returns a d × d matrix.</p>
<p>For a word x: [T(x)]i,j =    E(ui · vx + ai), if j = i (self-loop) E(wi · vx + bi), if j = i + 1 0, otherwise, (3) where ui and wi are vectors of parameters, ai and bi are scalar parameters, vx is a fixed pre-trained word vector for x, 4 and E is an encoding function, typically the identity function or sigmoid.</p>
<p>-transitions are also parameterized, but don’t consume a token and depend only on the current state: [T()]i,j = ( E(ci), if j = i + 1 0, otherwise, (4) where ci is a scalar parameter.5</p>
<p>As we have only three non-zero diagonals in total, the matrix multiplications in Equation 2 can be implemented using vector operations, and the overall runtimes of Forward and Viterbi are reduced to O(dn). 6</p>
<h4><span id="words-vs-wildcards">Words vs. wildcards.</span></h4><p>Traditional hard patterns distinguish between words and wildcards.</p>
<p>Our model does not explicitly capture the notion of either, but the transition weight function can be interpreted in those terms.</p>
<p>Each transition is a logistic regression over the next word vector vx.</p>
<p>For example, for a main path out of state i, T has two parameters, wi and bi .</p>
<p>If wi has large magnitude and is close to the word vector for some word y (e.g., wi ≈ 100vy), and bi is a large negative bias (e.g., bi ≈ −100), then the transition is essentially matching the specific word y.</p>
<p>Whereas if wi has small magnitude (wi ≈ 0) and bi is a large positive bias (e.g., bi ≈ 100), then the transition is ignoring the current token and matching a wildcard.7 The transition could also be something in between, for instance by focusing on specific dimensions of a word’s meaning encoded in the vector, such as POS or semantic features like animacy or concreteness (Rubinstein et al., 2015; Tsvetkov et al., 2015).</p>
<h4><span id="32-scoring-documents">3.2 Scoring Documents</span></h4><p>So far we described how to calculate how well a pattern matches a token span exactly (consuming the whole span).</p>
<p>To score a complete document, we prefer a score that aggregates over all matches on subspans of the document (similar to “search” instead of “match” in regular expression parlance).</p>
<p>We still assume a single pattern.</p>
<p>Either the Forward algorithm can be used to calculate the expected count of the pattern in the document, P 1≤i≤j≤n pspan(xi:j ), or Viterbi to calculate sdoc(x) = max1≤i≤j≤n sspan(xi:j ), the score of the highest-scoring match.</p>
<p>In short documents, we expect patterns to typically occur at most once, so in our experiments we choose the Viterbi algorithm, i.e., the max-product semiring.</p>
<h4><span id="implementation-details">Implementation details.</span></h4><p>We give the specific recurrences we use to score documents in a single pass with this model. We define:</p>
<p>[maxmul(A, B)]i,j = max k Ai,kBk,j .</p>
<p>(5) We also define the following for taking zero or one -transitions: eps (h) = maxmul (h, max(I, T())) (6) where max is element-wise max.</p>
<p>We maintain a row vector ht at each token:8 h0 = eps(π &gt;), (7a) ht+1 = max (eps(maxmul (ht , T(xt+1))), h0), (7b) and then extract and aggregate END state values: st = maxmul (ht , η), (8a) sdoc = max 1≤t≤n st .</p>
<p>(8b) [ht ]i represents the score of the best path through the pattern that ends in state i after consuming t tokens.</p>
<p>By including h0 in Equation 7b, we are accounting for spans that start at time t + 1. st is the maximum of the exact match scores for all spans ending at token t. And sdoc is the maximum score of any subspan in the document.</p>
<h3><span id="33-aggregating-multiple-patterns">3.3 Aggregating Multiple Patterns</span></h3><p>We describe how k patterns are aggregated to score a document.</p>
<p>These k patterns give k different sdoc scores for the document, which are stacked into a vector z ∈ R k and constitute the final document representation of SoPa.</p>
<p>This vector representation can be viewed as a feature vector.</p>
<p>In this paper, we feed it into a multilayer perceptron (MLP), culminating in a softmax to give a probability distribution over document labels.</p>
<p>We minimize cross-entropy, allowing the SoPa and MLP parameters to be learned end-to-end.</p>
<p>SoPa uses a total of (2e + 3)dk parameters, where e is the word embedding dimension, d is the number of states and k is the number of patterns.</p>
<p>For comparison, an LSTM with a hidden dimension of h has 4((e + 1)h + h 2 ).</p>
<p>In Section 6 we show that SoPa consistently uses fewer parameters than a BiLSTM baseline to achieve its best result.</p>
<h3><span id="34-sopa-as-an-rnn-sopa-can-be-considered-an-rnn">3.4 SoPa as an RNN SoPa can be considered an RNN.</span></h3><p>As shown in Section 3.2, a single pattern with d states has a hidden state vector of size d. Stacking the k hidden state vectors of k patterns into one vector of size k × d can be thought of as the hidden state of our model.</p>
<p>This hidden state is, like in any other RNN, dependent of the input and the previous state.</p>
<p>Using selfloops, the hidden state at time point i can in theory depend on the entire history of tokens up to xi (see Figure 2 for illustration).</p>
<p>We do want to discourage the model from following too many self-loops, only doing so if it results in a better fit with the remainder of the pattern.</p>
<p>To do this we use the sigmoid function as our encoding function E (see Equation 3), which means that all transitions have scores strictly less than 1.</p>
<p>This works to keep pattern matches close to their intended length.</p>
<p>Using other encoders, such as the identity function, can result in different dynamics, potentially encouraging rather than discouraging self-loops.</p>
<p>Although even single-layer RNNs are Turing complete (Siegelmann and Sontag, 1995), SoPa’s expressive power depends on the semiring.</p>
<p>When a WFSA is thought of as a function from finite sequences of tokens to semiring values, it is restricted to the class of functions known as rational series (Schutzenberger ¨ , 1961; Droste and Gastin, 1999; Sakarovitch, 2009).9 It is unclear how limiting this theoretical restriction is in practice, especially when SoPa is used as a component in a larger network.</p>
<p>We defer the investigation of the exact computational properties of SoPa to future work.</p>
<p>In the next section, we show that SoPa is an extension of a one-layer CNN, and hence more expressive.</p>
<h3><span id="4-sopa-as-a-cnn-extension">4 SoPa as a CNN Extension</span></h3><p>A convolutional neural network (CNN; LeCun, 1998) moves a fixed-size sliding window over the document, producing a vector representation for each window.</p>
<p>These representations are then often summed, averaged, or max-pooled to produce a document-level representation (Kim, 2014; Yin and Schutze ¨ , 2015).</p>
<p>In this section, we show that SoPa is an extension of one-layer, max-pooled CNNs.</p>
<blockquote>
<p>Figure 2: State activations of two patterns as they score a document. pattern1 (length three) matches on “in years”. pattern2 (length five) matches on “funniest and most likeable book”, using a self-loop to consume the token “most”. Active states in the best match are marked with arrow cursors.</p>
</blockquote>
<p>To recover a CNN from a soft pattern with d+ 1 states, we first remove self-loops and -transitions, retaining only the main path transitions.</p>
<p>We also use the identity function as our encoder E (Equation 3), and use the max-sum semiring.</p>
<p>With only main path transitions, the network will not match any span that is not exactly d tokens long.</p>
<p>Using max-sum, spans of length d will be assigned the score: sspan(xi:i+d) =X d−1 j=0 wj · vxi+j + bj , (9a) =w0:d · vxi:i+d + X d−1 j=0 bj , (9b) where w0:d = [w&gt; 0 ; .</p>
<p>; w&gt; d−1 ] &gt;, vxi:i+d = [v &gt; xi ; .</p>
<p>; v &gt; xi+d−1 ] &gt;.</p>
<p>Rearranged this way, we recognize the span score as an affine transformation of the concatenated word vectors vxi:i+d .</p>
<p>If we use k patterns, then together their span scores correspond to a linear filter with window size d and output dimension k. 10 A single pattern’s score for a document is: sdoc(x) = max 1≤i≤n−d+1 sspan(xi:i+d).</p>
<p>(10) The max in Equation 10 is calculated for each pattern independently, corresponding exactly to element-wise max-pooling of the CNN’s output layer.</p>
<p>Based on the equivalence between this impoverished version of SoPa and CNNs, we conclude that one-layer CNNs are learning an even more 10This variant of SoPa has d bias parameters, which correspond to only a single bias parameter in a CNN.</p>
<p>The redundant biases may affect optimization but are an otherwise unimportant difference.</p>
<p>restricted class of WFSAs (linear-chain WFSAs) that capture only fixed-length patterns.</p>
<p>One notable difference between SoPa and arbitrary CNNs is that in general CNNs can use any filter (like an MLP over vxi:i+d , for example).</p>
<p>In contrast, in order to efficiently pool over flexiblelength spans, SoPa is restricted to operations that follow the semiring laws.11 As a model that is more flexible than a one-layer CNN, but (arguably) less expressive than many RNNs, SoPa lies somewhere on the continuum between these two approaches.</p>
<p>Continuing to study the bridge between CNNs and RNNs is an exciting direction for future research.</p>
<h3><span id="5-experiments">5 Experiments</span></h3><p>To evaluate SoPa, we apply it to text classification tasks.</p>
<p>Below we describe our datasets and baselines.</p>
<p>More details can be found in Appendix A. </p>
<h4><span id="datasets">Datasets.</span></h4><p>We experiment with three binary classification datasets.</p>
<p>• SST.</p>
<p>The Stanford Sentiment Treebank (Socher et al., 2013) 12 contains roughly 10K movie reviews from Rotten Tomatoes,13 labeled on a scale of 1–5.</p>
<p>We consider the binary task, which considers 1 and 2 as negative, and 4 and 5 as positive (ignoring 3s).</p>
<p>It is worth noting that this dataset also contains syntactic phrase level annotations, providing a sentiment label to parts of   sentences.</p>
<p>In order to experiment in a realistic setup, we only consider the complete sentences, and ignore syntactic annotations at train or test time.</p>
<p>The number of training/development/test sentences in the dataset is 6,920/872/1,821.</p>
<p>• Amazon.</p>
<p>The Amazon Review Corpus (McAuley and Leskovec, 2013) 14 contains electronics product reviews, a subset of a larger review dataset.</p>
<p>Each document in the dataset contains a review and a summary.</p>
<p>Following Yogatama et al.</p>
<p>(2015), we only use the reviews part, focusing on positive and negative reviews.</p>
<p>The number of training/development/test samples is 20K/5K/25K.</p>
<p>• ROC.</p>
<p>The ROC story cloze task (Mostafazadeh et al., 2016) is a story understanding task.15 The task is composed of four-sentence story prefixes, followed by two competing endings: one that makes the joint five-sentence story coherent, and another that makes it incoherent.</p>
<p>Following Schwartz et al.</p>
<p>(2017), we treat it as a style detection task: we treat all “right” endings as positive samples and all “wrong” ones as negative, and we ignore the story prefix.</p>
<p>We split the development set into train and development (of sizes 3,366 and 374 sentences, respectively), and take the test set as-is (3,742 sentences).</p>
<h4><span id="reduced-training-data">Reduced training data.</span></h4><p>In order to test our model’s ability to learn from small datasets, we also randomly sample 100, 500, 1,000 and 2,500 SST training instances and 100, 500, 1,000, 2,500, 5,000, and 10,000 Amazon training instances.</p>
<p>Development and test sets remain the same.</p>
<h4><span id="baselines">Baselines.</span></h4><p>We compare to four baselines: a BiLSTM, a one-layer CNN, DAN (a simple alternative to RNNs) and a feature-based classifier trained with hard-pattern features.</p>
<p>• BiLSTM.</p>
<p>Bidirectional LSTMs have been successfully used in the past for text classification tasks (Zhou et al., 2016).</p>
<p>We learn a one-layer BiLSTM representation of the document, and feed the average of all hidden states to an MLP.</p>
<p>• CNN.</p>
<p>CNNs are particularly useful for text classification (Kim, 2014).</p>
<p>We train a one-layer CNN with max-pooling, and feed the resulting representation to an MLP.</p>
<p>• DAN.</p>
<p>We learn a deep averaging network with word dropout (Iyyer et al., 2015), a simple but strong text-classification baseline.</p>
<p>• Hard.</p>
<p>We train a logistic regression classifier with hard-pattern features.</p>
<p>Following Tsur et al.</p>
<p>(2010), we replace low frequency words with a special wildcard symbol.</p>
<p>We learn sequences of 1–6 concrete words, where any number of wildcards can come between two adjacent words.</p>
<p>We consider words occurring with frequency of at least 0.01% of our training set as concrete words, and words occurring in frequency 1% or less as wildcards.1</p>
<h4><span id="number-of-patterns">Number of patterns.</span></h4><p>SoPa requires specifying the number of patterns to be learned, and their lengths.</p>
<p>Preliminary experiments showed that the model doesn’t benefit from more than a few dozen patterns.</p>
<p>We experiment with several configurations of patterns of different lengths, generally considering 0, 10 or 20 patterns of each pattern length between 2–7.</p>
<p>The total number of patterns learned ranges between 30–70.17 6 Results Table 1 shows our main experimental results.</p>
<p>In two of the cases (SST and ROC), SoPa outperforms all models.</p>
<p>On Amazon, SoPa performs within 0.3 points of CNN and BiLSTM, and outperforms the other two baselines.</p>
<p>The table also shows the number of parameters used by each model for each task.</p>
<p>Given enough data, models with more parameters should be expected to perform better.</p>
<p>However, SoPa performs better or roughly the same as a BiLSTM, which has 3–6 times as many parameters.</p>
<p>Figure 3 shows a comparison of all models on the SST and Amazon datasets with varying training set sizes.</p>
<p>SoPa is substantially outperforming all baselines, in particular BiLSTM, on small datasets (100 samples).</p>
<p>This suggests that SoPa is better fit to learn from small datasets.</p>
<h4><span id="ablation-analysis">Ablation analysis.</span></h4><p>Table 1 also shows an ablation of the differences between SoPa and CNN: max-product semiring with sigmoid vs. max-sum semiring with identity, self-loops, and -transitions.</p>
<p>The last line is equivalent to a CNN with multiple window sizes.</p>
<p>Interestingly, the most notable difference between SoPa and CNN is the semiring and encoder function, while  transitions and self-loops have little effect on performance.18 7 Interpretability We turn to another key aspect of SoPa—its interpretability.</p>
<p>We start by demonstrating how we interpret a single pattern, and then describe how to interpret the decisions made by downstream classifiers that rely on SoPa—in this case, a sentence classifier.</p>
<p>Importantly, these visualization techniques are equally applicable to CNNs.</p>
<h4><span id="interpreting-a-single-pattern">Interpreting a single pattern.</span></h4><p>In order to visualize a pattern, we compute the pattern matching scores with each phrase in our training dataset, and select the k phrases with the highest scores.</p>
<p>Table 2 shows examples of six patterns learned using the best SoPa model on the SST dataset, as represented by their five highest scoring phrases in the training set.</p>
<p>A few interesting trends can be observed from these examples.</p>
<p>First, it seems our patterns encode semantically coherent expressions.</p>
<p>A large portion of them correspond to sentiment (the five top examples in the table), but others capture different semantics, e.g., time expressions.</p>
<p>Second, it seems our patterns are relatively soft, and allow lexical flexibility.</p>
<p>While some patterns do seem to fix specific words, e.g., “of” in the first example or “minutes” in the last one, even in those cases some of the top matching spans replace these words with other, similar words (“with” and “halfhour”, respectively).</p>
<p>Encouraging SoPa to have more concrete words, e.g., by jointly learning the word vectors, might make SoPa useful in other contexts, particularly as a decoder.</p>
<p>We defer this direction to future work.</p>
<p>Finally, SoPa makes limited but non-negligible use of self-loops and epsilon steps.</p>
<p>Interestingly, the second example shows that one of the patterns had an -transition at the same place in every phrase.</p>
<p>This demonstrates a different function of -transitions than originally designed—they allow a pattern to effectively shorten itself, by learning a high -transition parameter for a certain state.</p>
<h4><span id="interpreting-a-document">Interpreting a document.</span></h4><p>SoPa provides an interpretable representation of a document—a vector of the maximal matching score of each pattern with any span in the document.</p>
<p>To visualize the decisions of our model for a given document, we can observe the patterns and corresponding phrases that score highly within it.</p>
<p>To understand which of the k patterns contributes most to the classification decision, we apply a leave-one-out method.</p>
<p>We run the forward method of the MLP layer in SoPa k times, each time zeroing-out the score of a different pattern p. The difference between the resulting score and the original model score is considered p’s contribution.</p>
<p>We then consider the highest contributing patterns, and attach each one with its highest scoring phrase in that document.</p>
<p>Table 3 shows example texts along with their most positive and negative contributing phrases.</p>
<h2><span id="related-work">Related Work</span></h2><p>Weighted finite-state automata.</p>
<p>WFSAs and hidden Markov models19 were once popular in automatic speech recognition (Hetherington, 2004; Moore et al., 2006; Hoffmeister et al., 2012) and remain popular in morphology (Dreyer, 2011; Cotterell et al., 2015).</p>
<p>Most closely related to this work, neural networks have been combined with weighted finite-state transducers to do morphological reinflection (Rastogi et al., 2016).</p>
<p>These prior works learn a single FSA or FST, whereas our model learns a collection of simple but complementary FSAs, together encoding a sequence.</p>
<p>We are the first to incorporate neural networks both before WFSAs (in their transition scoring functions), and after (in the function that turns their vector of scores into a final prediction), to produce an expressive model that remains interpretable.</p>
<h3><span id="recurrent-neural-networks">Recurrent neural networks.</span></h3><p>The ability of RNNs to represent arbitrarily long sequences of embedded tokens has made them attractive to NLP researchers.</p>
<p>The most notable variants, the long short-term memory (LSTM; Hochreiter and Schmidhuber, 1997) and gated recurrent units (GRU; Cho et al., 2014), have become ubiquitous in NLP algorithms (Goldberg, 2016).</p>
<p>Recently, several works introduced simpler versions of RNNs, such as recurrent additive networks (Lee et al., 2017) and Quasi-RNNs (Bradbury et al., 2017).</p>
<p>Like SoPa, these models can be seen as points along the bridge between RNNs and CNNs.</p>
<p>Other works have studied the expressive power of RNNs, in particular in the context of WFSAs or HMMs (Cleeremans et al., 1989; Giles et al., 1992; Visser et al., 2001; Chen et al., 2018).</p>
<p>In this work we relate CNNs to WFSAs, showing that a one-layer CNN with max-pooling can be simulated by a collection of linear-chain WFSAs.</p>
<h3><span id="convolutional-neural-networks">Convolutional neural networks.</span></h3><p>CNNs are prominent feature extractors in NLP, both for generating character-based embeddings (Kim et al., 2016), and as sentence encoders for tasks like text classification (Yin and Schutze ¨ , 2015) and machine translation (Gehring et al., 2017).</p>
<p>Similarly to SoPa, several recently introduced variants of CNNs support varying window sizes by either allowing several fixed window sizes (Yin and Schutze ¨ , 2015) or by supporting non-consecutive n-gram matching (Lei et al., 2015; Nguyen and Grishman, 2016).</p>
<h3><span id="neural-networks-and-patterns">Neural networks and patterns.</span></h3><p>Some works used patterns as part of a neural network.</p>
<p>Schwartz et al. (2016) used pattern contexts for estimating word embeddings, showing improved word similarity results compared to bag-of-word contexts.</p>
<p>Shwartz et al. (2016) designed an LSTM representation for dependency patterns, using them to detect hypernymy relations.</p>
<p>Here, we learn patterns as a neural version of WFSAs.</p>
<p>Interpretability. There have been several efforts to interpret neural models.</p>
<p>The weights of the attention mechanism (Bahdanau et al., 2015) are often used to display the words that are most significant for making a prediction.</p>
<p>LIME (Ribeiro et al., 2016) is another approach for visualizing neural models (not necessarily textual).</p>
<p>Yogatama and Smith (2014) introduced structured sparsity, which encodes linguistic information into the regularization of a model, thus allowing to visualize the contribution of different bag-of-word features.</p>
<p>Other works jointly learned to encode text and extract the span which best explains the model’s prediction (Yessenalina et al., 2010; Lei et al., 2016).</p>
<p>Li et al. (2016) and Kad´ ar et al. ´ (2017) suggested a method that erases pieces of the text in order to analyze their effect on a neural model’s decisions.</p>
<p>Finally, several works presented method to visualize deep CNNs (Zeiler and Fergus, 2014; Simonyan et al., 2014; Yosinski et al., 2015), focusing on visualizing the different layers of the network, mainly in the context of image and video understanding.</p>
<p>We believe these two types of research approaches are complementary: inventing general purpose visualization tools for existing black-box models on the one hand, and on the other, designing models like SoPa that are interpretable by construction.</p>
<h2><span id="conclusion">Conclusion</span></h2><p>We introduced SoPa, a novel model that combines neural representation learning with WFSAs.</p>
<p>We showed that SoPa is an extension of a one-layer CNN.</p>
<p>It naturally models flexible-length spans with insertion and deletion, and it can be easily customized by swapping in different semirings.</p>
<p>SoPa performs on par with or strictly better than four baselines on three text classification tasks, while requiring fewer parameters than the stronger baselines.</p>
<p>On smaller training sets, SoPa outperforms all four baselines.</p>
<p>As a simple version of an RNN, which is more expressive than one-layer CNNs, we hope that SoPa will encourage future research on the bridge between these two mechanisms.</p>
<p>To facilitate such research, we release our implementation at <a href="https://github.com/Noahs-ARK/soft_patterns" target="_blank" rel="noopener">https://github.com/Noahs-ARK/soft_patterns</a>.</p>

            

        </div>
    </div>
    <div id="post-footer" class="post-footer main-content-wrap">
        
        
            <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    <a class="post-action-btn btn btn--disabled">
                
                    <i class="fa fa-angle-left" aria-hidden="true"></i>
                    <span class="hide-xs hide-sm text-small icon-ml">上一篇</span>
                </a>
            </li>
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="/archives/24456aaf.html" data-tooltip="Natural language processing - foundations and algorithms" aria-label="下一篇: Natural language processing - foundations and algorithms">
                
                    <span class="hide-xs hide-sm text-small icon-mr">下一篇</span>
                    <i class="fa fa-angle-right" aria-hidden="true"></i>
                </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions" aria-label="Share this post">
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/archives/ccb21e16.html" title="分享到 Facebook">
                    <i class="fab fa-facebook" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=http://yoursite.com/archives/ccb21e16.html" title="分享到 Twitter">
                    <i class="fab fa-twitter" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="https://plus.google.com/share?url=http://yoursite.com/archives/ccb21e16.html" title="分享到 Google+">
                    <i class="fab fa-google-plus" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="http://service.weibo.com/share/share.php?&amp;title=http://yoursite.com/archives/ccb21e16.html" title="分享到 Weibo">
                    <i class="fab fa-weibo" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="http://connect.qq.com/widget/shareqq/index.html?url=http://yoursite.com/archives/ccb21e16.html&amp;title=SoPa: Bridging CNNs, RNNs, and Weighted Finite-State Machines" title="分享到 QQ">
                    <i class="fab fa-qq" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="http://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=http://yoursite.com/archives/ccb21e16.html" title="分享到 Qzone">
                    <i class="fa fa-star" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="http://widget.renren.com/dialog/share?resourceUrl=http://yoursite.com/archives/ccb21e16.html" title="分享到 Renren">
                    <i class="fab fa-renren" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#table-of-contents" aria-label="目录">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


        
        
            
                <div id="cyReward" role="cylabs" data-use="reward"></div>
<div id="cyPoll" role="cylabs" data-use="poll"></div>
<div id="cyEmoji" role="cylabs" data-use="emoji"></div>
<div id="cyQing" role="cylabs" data-use="qing"></div>
<div id="cyHotusers" role="cylabs" data-use="hotusers"></div>
<div id="cyReping" role="cylabs" data-use="reping"></div>
<div id="cyBarrage" role="cylabs" data-use="barrage"></div>
<div id="SOHUCS" sid="SoPa: Bridging CNNs, RNNs, and Weighted Finite-State Machines"></div>


            
        
    </div>
</article>


                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2019 天津亦友拾光文化传播有限公司. 津ICP备18005385号
    </span>
</footer>

            </div>
            
                <div id="bottom-bar" class="post-bottom-bar" data-behavior="4">
                    <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    <a class="post-action-btn btn btn--disabled">
                
                    <i class="fa fa-angle-left" aria-hidden="true"></i>
                    <span class="hide-xs hide-sm text-small icon-ml">上一篇</span>
                </a>
            </li>
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="/archives/24456aaf.html" data-tooltip="Natural language processing - foundations and algorithms" aria-label="下一篇: Natural language processing - foundations and algorithms">
                
                    <span class="hide-xs hide-sm text-small icon-mr">下一篇</span>
                    <i class="fa fa-angle-right" aria-hidden="true"></i>
                </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions" aria-label="Share this post">
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/archives/ccb21e16.html" title="分享到 Facebook">
                    <i class="fab fa-facebook" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=http://yoursite.com/archives/ccb21e16.html" title="分享到 Twitter">
                    <i class="fab fa-twitter" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="https://plus.google.com/share?url=http://yoursite.com/archives/ccb21e16.html" title="分享到 Google+">
                    <i class="fab fa-google-plus" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="http://service.weibo.com/share/share.php?&amp;title=http://yoursite.com/archives/ccb21e16.html" title="分享到 Weibo">
                    <i class="fab fa-weibo" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="http://connect.qq.com/widget/shareqq/index.html?url=http://yoursite.com/archives/ccb21e16.html&amp;title=SoPa: Bridging CNNs, RNNs, and Weighted Finite-State Machines" title="分享到 QQ">
                    <i class="fab fa-qq" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="http://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=http://yoursite.com/archives/ccb21e16.html" title="分享到 Qzone">
                    <i class="fa fa-star" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="http://widget.renren.com/dialog/share?resourceUrl=http://yoursite.com/archives/ccb21e16.html" title="分享到 Renren">
                    <i class="fab fa-renren" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#table-of-contents" aria-label="目录">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


                </div>
                
    <div id="share-options-bar" class="share-options-bar" data-behavior="4">
        <i id="btn-close-shareoptions" class="fa fa-times"></i>
        <ul class="share-options">
            
                
                
                <li class="share-option">
                    <a class="share-option-btn" target="new" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/archives/ccb21e16.html">
                        <i class="fab fa-facebook" aria-hidden="true"></i><span>分享到 Facebook</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a class="share-option-btn" target="new" href="https://twitter.com/intent/tweet?text=http://yoursite.com/archives/ccb21e16.html">
                        <i class="fab fa-twitter" aria-hidden="true"></i><span>分享到 Twitter</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a class="share-option-btn" target="new" href="https://plus.google.com/share?url=http://yoursite.com/archives/ccb21e16.html">
                        <i class="fab fa-google-plus" aria-hidden="true"></i><span>分享到 Google+</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a class="share-option-btn" target="new" href="http://service.weibo.com/share/share.php?&amp;title=http://yoursite.com/archives/ccb21e16.html">
                        <i class="fab fa-weibo" aria-hidden="true"></i><span>分享到 Weibo</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a class="share-option-btn" target="new" href="http://connect.qq.com/widget/shareqq/index.html?url=http://yoursite.com/archives/ccb21e16.html&amp;title=SoPa: Bridging CNNs, RNNs, and Weighted Finite-State Machines">
                        <i class="fab fa-qq" aria-hidden="true"></i><span>分享到 QQ</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a class="share-option-btn" target="new" href="http://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=http://yoursite.com/archives/ccb21e16.html">
                        <i class="fa fa-star" aria-hidden="true"></i><span>分享到 Qzone</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a class="share-option-btn" target="new" href="http://widget.renren.com/dialog/share?resourceUrl=http://yoursite.com/archives/ccb21e16.html">
                        <i class="fab fa-renren" aria-hidden="true"></i><span>分享到 Renren</span>
                    </a>
                </li>
            
        </ul>
    </div>


            
        </div>
        


    
        
    

<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-times"></i>
        </div>
        
            <img id="about-card-picture" src="http://cdn.yiyouls.com/profile.png" alt="作者的图片">
        
            <h4 id="about-card-name">天津亦友拾光文化传播有限公司</h4>
        
            <div id="about-card-bio"><p>浙江大学在读</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br>
                <p>博士研究生</p>

            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker-alt"></i>
                <br>
                Hangzhou,Zhejiang
            </div>
        
    </div>
</div>

        
            <div id="algolia-search-modal" class="modal-container">
    <div class="modal">
        <div class="modal-header">
            <span class="close-button"><i class="fa fa-times"></i></span>
            <a href="https://algolia.com" target="_blank" rel="noopener" class="searchby-algolia text-color-light link-unstyled">
                <span class="searchby-algolia-text text-color-light text-small">by</span>
                <img class="searchby-algolia-logo" src="https://www.algolia.com/static_assets/images/press/downloads/algolia-light.svg">
            </a>
            <i class="search-icon fa fa-search"></i>
            <form id="algolia-search-form">
                <input type="text" id="algolia-search-input" name="search" class="form-control input--large search-input" placeholder="Search ">
            </form>
        </div>
        <div class="modal-body">
            <div class="no-result text-color-light text-center">没有找到文章</div>
            <div class="results">
                
                <div class="media">
                    
                    <div class="media-left">
                        <a class="link-unstyled" href="http://yoursite.com/archives/bc1e.html">
                            <img class="media-image" src="http://cdn.yiyouls.com/image_1.png" width="90" height="90">
                        </a>
                    </div>
                    
                    <div class="media-body">
                        <a class="link-unstyled" href="http://yoursite.com/archives/bc1e.html">
                            <h3 class="media-heading">什么是机器学习(Machine Learning)？</h3>
                        </a>
                        <span class="media-meta">
                            <span class="media-date text-small">
                                
                                    2019年1月16日
                                
                            </span>
                        </span>
                        <div class="media-content hide-xs font-merryweather"><p>机器学习与深度学习基础教程－从零开始</p>
<p>本系列教程是为初学者定制，涵盖并解释了深度学习和人工神经网络的基本概念</p></div>
                    </div>
                    <div style="clear:both;"></div>
                    <hr>
                </div>
                
                <div class="media">
                    
                    <div class="media-left">
                        <a class="link-unstyled" href="http://yoursite.com/archives/f498.html">
                            <img class="media-image" src="http://cdn.yiyouls.com/image_2.png" width="90" height="90">
                        </a>
                    </div>
                    
                    <div class="media-body">
                        <a class="link-unstyled" href="http://yoursite.com/archives/f498.html">
                            <h3 class="media-heading">什么是深度学习(Deep Learning)？</h3>
                        </a>
                        <span class="media-meta">
                            <span class="media-date text-small">
                                
                                    2019年1月16日
                                
                            </span>
                        </span>
                        <div class="media-content hide-xs font-merryweather"><p>机器学习与深度学习基础教程</p>
<p>这篇文章将要回答什么是深度学习这个问题．</p>
<p>整个系列课程会涵盖深度学习领域的众多主题，我们会用很多篇文章来充分解释这些课题，以及它们的应用领域和技术实现。</p></div>
                    </div>
                    <div style="clear:both;"></div>
                    <hr>
                </div>
                
                <div class="media">
                    
                    <div class="media-left">
                        <a class="link-unstyled" href="http://yoursite.com/archives/97f0.html">
                            <img class="media-image" src="http://cdn.yiyouls.com/web.png" width="90" height="90">
                        </a>
                    </div>
                    
                    <div class="media-body">
                        <a class="link-unstyled" href="http://yoursite.com/archives/97f0.html">
                            <h3 class="media-heading">使用hexo在Github上免费搭建功能丰富的个人博客网站</h3>
                        </a>
                        <span class="media-meta">
                            <span class="media-date text-small">
                                
                                    2019年1月18日
                                
                            </span>
                        </span>
                        <div class="media-content hide-xs font-merryweather"><p>这个系列博文将带你一步步搭建起自己的个人博客网站，包含tranquilpeak主题模板配置，站内搜索，live2d插件，mathjax数学公式插件，＂千牛云＂云加速，＂畅言＂评论区添加，打赏设置等等高阶功能．</p></div>
                    </div>
                    <div style="clear:both;"></div>
                    <hr>
                </div>
                
                <div class="media">
                    
                    <div class="media-left">
                        <a class="link-unstyled" href="http://yoursite.com/archives/7cef.html">
                            <img class="media-image" src="http://cdn.yiyouls.com/Octocat.png" width="90" height="90">
                        </a>
                    </div>
                    
                    <div class="media-body">
                        <a class="link-unstyled" href="http://yoursite.com/archives/7cef.html">
                            <h3 class="media-heading">如何在Github上免费开通一个Github Page作为个人主页</h3>
                        </a>
                        <span class="media-meta">
                            <span class="media-date text-small">
                                
                                    2019年1月19日
                                
                            </span>
                        </span>
                        <div class="media-content hide-xs font-merryweather"><p>为什么要选择在Github上建站，原因很简单，它是免费的．</p>
<p>只要你按照下面的六步走，就可以轻轻松松发布自己的网站，你不需要有任何关于编程或者网站建设的基础知识．所有的前提只是，你要有一个github账号．</p></div>
                    </div>
                    <div style="clear:both;"></div>
                    <hr>
                </div>
                
                <div class="media">
                    
                    <div class="media-left">
                        <a class="link-unstyled" href="http://yoursite.com/archives/b5e2.html">
                            <img class="media-image" src="http://cdn.yiyouls.com/hexologo.png" width="90" height="90">
                        </a>
                    </div>
                    
                    <div class="media-body">
                        <a class="link-unstyled" href="http://yoursite.com/archives/b5e2.html">
                            <h3 class="media-heading">11大最常用静态网站建站工具，你用过几个？</h3>
                        </a>
                        <span class="media-meta">
                            <span class="media-date text-small">
                                
                                    2019年1月19日
                                
                            </span>
                        </span>
                        <div class="media-content hide-xs font-merryweather"><p>在进行下一步操作之前，你首先要明白，你通过我们上篇博文所介绍的方式建立的网站，是一种静态网站，这个是最近特别流行的静态网站搭建博客的技术，非常适合让站长专注于内容创作而非网站本身的维护．</p>
<p>为什么不用动态网站例如大名鼎鼎的wordpress呢？</p></div>
                    </div>
                    <div style="clear:both;"></div>
                    <hr>
                </div>
                
                <div class="media">
                    
                    <div class="media-left">
                        <a class="link-unstyled" href="http://yoursite.com/archives/7770.html">
                            <img class="media-image" src="http://cdn.yiyouls.com/anns.png" width="90" height="90">
                        </a>
                    </div>
                    
                    <div class="media-body">
                        <a class="link-unstyled" href="http://yoursite.com/archives/7770.html">
                            <h3 class="media-heading">什么是人工神经网络ANNs?</h3>
                        </a>
                        <span class="media-meta">
                            <span class="media-date text-small">
                                
                                    2019年1月25日
                                
                            </span>
                        </span>
                        <div class="media-content hide-xs font-merryweather"><p>什么是人工神经网络(artificial neural network)?</p>
<p>在上一篇文章中我们把深度学习定义为机器学习的子领域，深度学习所用到的算法是受大脑神经网络的结构和功能的启发．因为这个浅显的的原因，深度学习中使用的模型被称为人工神经网络（ANN）。</p></div>
                    </div>
                    <div style="clear:both;"></div>
                    <hr>
                </div>
                
                <div class="media">
                    
                    <div class="media-left">
                        <a class="link-unstyled" href="http://yoursite.com/archives/da21.html">
                            <img class="media-image" src="http://cdn.yiyouls.com/ceng.png" width="90" height="90">
                        </a>
                    </div>
                    
                    <div class="media-body">
                        <a class="link-unstyled" href="http://yoursite.com/archives/da21.html">
                            <h3 class="media-heading">如何理解神经网络中层(layer)的概念？</h3>
                        </a>
                        <span class="media-meta">
                            <span class="media-date text-small">
                                
                                    2019年1月25日
                                
                            </span>
                        </span>
                        <div class="media-content hide-xs font-merryweather"><p>如何理解神经网络中层(layer)的概念?</p>
<p>我们在之前的文章中认识了只有三个层的神经网络，但在很多时候我们有多个隐藏层，这篇文章探讨如何深入理解神经网络中层的概念，并提供一个多层神经网络的构建方法．</p></div>
                    </div>
                    <div style="clear:both;"></div>
                    <hr>
                </div>
                
                <div class="media">
                    
                    <div class="media-left">
                        <a class="link-unstyled" href="http://yoursite.com/archives/10fe.html">
                            <img class="media-image" src="http://cdn.yiyouls.com/function.png" width="90" height="90">
                        </a>
                    </div>
                    
                    <div class="media-body">
                        <a class="link-unstyled" href="http://yoursite.com/archives/10fe.html">
                            <h3 class="media-heading">如何理解神经网络中的激活函数(ativation functions)?</h3>
                        </a>
                        <span class="media-meta">
                            <span class="media-date text-small">
                                
                                    2019年1月26日
                                
                            </span>
                        </span>
                        <div class="media-content hide-xs font-merryweather"><p>神经网络中的激活函数</p>
<p>这篇文章向大家解释激活函数到底是什么，以及如何在神经网络中使用激活函数．我会向大家介绍几种常见的激活函数，向大家展示这些激活函数如何在Keras中以代码的形式呈现．</p></div>
                    </div>
                    <div style="clear:both;"></div>
                    <hr>
                </div>
                
                <div class="media">
                    
                    <div class="media-left">
                        <a class="link-unstyled" href="http://yoursite.com/archives/4c0a.html">
                            <img class="media-image" src="http://cdn.yiyouls.com/sgd.png" width="90" height="90">
                        </a>
                    </div>
                    
                    <div class="media-body">
                        <a class="link-unstyled" href="http://yoursite.com/archives/4c0a.html">
                            <h3 class="media-heading">如何训练(train)人工神经网络？</h3>
                        </a>
                        <span class="media-meta">
                            <span class="media-date text-small">
                                
                                    2019年1月26日
                                
                            </span>
                        </span>
                        <div class="media-content hide-xs font-merryweather"><h4 id="神经网络的训练"><a href="#神经网络的训练" class="headerlink" title="神经网络的训练"></a>神经网络的训练</h4><p>这篇文章带大家学习如何训练一个人工神经网络．之前的文章中我们学习了如何构建一个简单的人工神经网络，当我们把一个神经网络配置好之后，下一步就是来训练它了．</p></div>
                    </div>
                    <div style="clear:both;"></div>
                    <hr>
                </div>
                
                <div class="media">
                    
                    <div class="media-left">
                        <a class="link-unstyled" href="http://yoursite.com/archives/b5f9.html">
                            <img class="media-image" src="http://cdn.yiyouls.com/learn.png" width="90" height="90">
                        </a>
                    </div>
                    
                    <div class="media-body">
                        <a class="link-unstyled" href="http://yoursite.com/archives/b5f9.html">
                            <h3 class="media-heading">人工神经网络是如何学习(learn)的?</h3>
                        </a>
                        <span class="media-meta">
                            <span class="media-date text-small">
                                
                                    2019年1月27日
                                
                            </span>
                        </span>
                        <div class="media-content hide-xs font-merryweather"><p>神经网络的学习过程</p>
<p>这篇文章我们来探讨人工神经网络是怎样学习的．</p>
<p>本篇内容涵盖损失loss，梯度gradient，学习率lerning rate，epoch，verbose，batch批处理等概念．</p></div>
                    </div>
                    <div style="clear:both;"></div>
                    <hr>
                </div>
                
            </div>
        </div>
        <div class="modal-footer">
            <p class="results-count text-medium" data-message-zero="没有找到文章" data-message-one="找到 1 篇文章" data-message-other="找到 {n} 篇文章">
                找到 30 篇文章
            </p>
        </div>
    </div>
</div>

        
        
<div id="cover" style="background-image:url('http://cdn.yiyouls.com/cover.jpg');"></div>
        <!--SCRIPTS-->
<script src="/assets/js/script-dbd16rvloemmuxdzniplmnxxvwoz24eya9wol0b7vvmlokgqsjivmb8dnscy.min.js"></script>
<!--SCRIPTS END-->

    
<script type="text/javascript"> 
(function(){ 
var appid = 'cyu2tTq0X'; 
var conf = 'prod_04b0d924b891393f35b9626442a6e448'; 
var width = window.innerWidth || document.documentElement.clientWidth; 
if (width < 960) { 
window.document.write('<script id="changyan_mobile_js" charset="utf-8" type="text/javascript" src="http://changyan.sohu.com/upload/mobile/wap-js/changyan_mobile.js?client_id=' + appid + '&conf=' + conf + '"><\/script>'); } else { var loadJs=function(d,a){var c=document.getElementsByTagName("head")[0]||document.head||document.documentElement;var b=document.createElement("script");b.setAttribute("type","text/javascript");b.setAttribute("charset","UTF-8");b.setAttribute("src",d);if(typeof a==="function"){if(window.attachEvent){b.onreadystatechange=function(){var e=b.readyState;if(e==="loaded"||e==="complete"){b.onreadystatechange=null;a()}}}else{b.onload=a}}c.appendChild(b)};loadJs("https://changyan.sohu.com/upload/changyan.js",function(){window.changyan.api.config({appid:appid,conf:conf})}); } })(); </script> 
<script type="text/javascript" charset="utf-8" src="https://changyan.itc.cn/js/lib/jquery.js"></script>
<script type="text/javascript" charset="utf-8" src="https://changyan.sohu.com/js/changyan.labs.https.js?appid=cyu2tTq0X"></script>
    


    <script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.14.1/moment-with-locales.min.js"></script>
    <script src="//cdn.jsdelivr.net/algoliasearch/3/algoliasearch.min.js"></script>
    <script>
        var algoliaClient = algoliasearch('JRXR2OL9UV', '2f9d9502c1e5ad12ac19a2180d9ac94d');
        var algoliaIndex = algoliaClient.initIndex('my-hexo-blog');
    </script>


    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script src="/live2dw/lib/L2Dwidget.min.js?0c58a1486de42ac6cc1c59c7d98ae887"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body>
</html>
